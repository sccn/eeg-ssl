{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "class PosEmb(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PosEmb, self).__init__()\n",
        "    self.conv = nn.Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
        "    self.conv = nn.utils.weight_norm(self.conv, name=\"weight\", dim=2)\n",
        "    self.activation = nn.GELU()\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = x[:, :, :-1]\n",
        "    return torch.permute(self.activation(x), (0,2,1))"
      ],
      "metadata": {
        "id": "96-bP8b98TtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mni7S0Iv5RbM"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class Wav2Vec2(nn.Module):\n",
        "  def __init__(self, K, S, class_dim):\n",
        "    super(FeatureEncoder, self).__init__()\n",
        "    self.conv0 = []\n",
        "    self.K = K\n",
        "    self.S = S\n",
        "    self.conv0.append(nn.Sequential(\n",
        "        nn.Conv1d(128, 512, kernel_size=(self.K[0],), stride=(self.S[0],), bias=False),\n",
        "        nn.GELU(),\n",
        "        nn.GroupNorm(512, 512, eps=1e-05, affine=True)\n",
        "    ))\n",
        "    self.conv1 = []\n",
        "    for i in range(4):\n",
        "      self.conv1.append(nn.Sequential(\n",
        "          nn.Conv1d(512, 512, kernel_size=(self.K[1],), stride=(self.S[1],), bias=False),\n",
        "          nn.GELU()\n",
        "      ))\n",
        "    self.conv2 = []\n",
        "    for i in range(2):\n",
        "      self.conv2.append(nn.Sequential(\n",
        "          nn.Conv1d(512, 512, kernel_size=(self.K[5],), stride=(self.S[5],), bias=False),\n",
        "          nn.GELU()\n",
        "        ))\n",
        "    self.extractor = nn.Sequential(*self.conv0, *self.conv1, *self.conv2)\n",
        "    self.projection = nn.Sequential(\n",
        "        nn.LayerNorm((512,), eps=1e-05, elementwise_affine=True),\n",
        "        nn.Linear(in_features=512, out_features=768, bias=True),\n",
        "        nn.Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "    self.pos_emb = PosEmb()\n",
        "    self.norm = nn.Sequential(\n",
        "        nn.LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
        "        nn.Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "    self.attention = []\n",
        "    for i in range(12):\n",
        "      self.attention.append(nn.MultiheadAttention(768, 1))\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Dropout(p=0.1, inplace=False),\n",
        "        nn.Linear(in_features=768, out_features=3072, bias=True),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(in_features=3072, out_features=768, bias=True),\n",
        "        nn.Dropout(p=0.1, inplace=False),\n",
        "        nn.LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "\n",
        "\n",
        "  def receptive_field(self):\n",
        "    St = 1\n",
        "    stride = self.S[::-1]\n",
        "    for s in stride:\n",
        "      St *= s\n",
        "    R = 1\n",
        "    i = 0\n",
        "    kernel = self.K[::-1]\n",
        "    for k in kernel:\n",
        "      R = R*stride[i] + (k - stride[i])\n",
        "      i += 1\n",
        "    return R, St\n",
        "\n",
        "  def feature_encoder(self, x):\n",
        "    x = self.extractor(x)\n",
        "    x = torch.permute(x, (0,2,1))\n",
        "    return torch.permute(self.projection(x), (0,2,1))\n",
        "\n",
        "  def context_encoder(self, x):\n",
        "    x = self.pos_emb(x) + torch.permute(x, (0,2,1))\n",
        "    x = self.norm(x)\n",
        "    for i in range(12):\n",
        "      x = self.attention[i](x, x, x)[0]\n",
        "    return torch.permute(self.feed_forward(x), (0,2,1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.feature_encoder(x)\n",
        "    x = torch.flatten(x, start_dim = 2)\n",
        "    x = self.context_encoder(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import os\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "class MaskedContrastiveLearningTask():\n",
        "    def __init__(self,\n",
        "                dataset: torch.utils.data.Dataset,\n",
        "                task_params={\n",
        "                    'mask_prob': 0.5\n",
        "                },\n",
        "                train_params={\n",
        "                    'num_epochs': 100,\n",
        "                    'batch_size': 10,\n",
        "                    'print_every': 10\n",
        "                },\n",
        "                verbose=False\n",
        "        ):\n",
        "        self.dataset = dataset\n",
        "        self.train_test_split()\n",
        "\n",
        "        self.train_params = train_params\n",
        "        self.mask_probability = task_params['mask_prob']\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.verbose=verbose\n",
        "\n",
        "    def train_test_split(self):\n",
        "        generator = torch.Generator().manual_seed(42)\n",
        "        self.dataset_train, self.dataset_val = torch.utils.data.random_split(self.dataset, [0.7,0.3], generator=generator)\n",
        "\n",
        "    def forward(self, model, x):\n",
        "        '''\n",
        "        Forward pass of the model\n",
        "        @parameter\n",
        "            model:  nn.Module   model\n",
        "            x    :  tensor      (N x C x T) batched raw input\n",
        "        @return\n",
        "            prediction:         (N x D x K) Batch-size embeddings of the model's guess for masked inputs\n",
        "            masked_latent:      (N x D x K) Batch-size embeddings of the feature encoder output of true masked inputs\n",
        "            foil_latents:       (N x D x K) Batch-size embeddings of the feature conder output of the foil inputs\n",
        "        '''\n",
        "        embeddings = model.feature_encoder(x) # N x D x K\n",
        "                                              # forward pass of feature encoder generate intermediary embeddings\n",
        "        if self.verbose:\n",
        "            print('feature encoder output shape', embeddings.shape)\n",
        "\n",
        "        # learned masked vector embedding\n",
        "        masked_vector_learned_embedding = torch.ones((embeddings.shape[0], embeddings.shape[1])) # N x D # TODO\n",
        "        if self.verbose:\n",
        "            print('learned masked embeddings shape', masked_vector_learned_embedding.shape)\n",
        "\n",
        "        # select from the sampled segment L masked inputs\n",
        "        masked_indices = np.random.choice(embeddings.shape[-1], size=(int(self.mask_probability*embeddings.shape[-1]),), replace=False)\n",
        "        if self.verbose:\n",
        "            print('masked indices shape', masked_indices.shape)\n",
        "        # replace the selected indices with the masked vector embedding\n",
        "        true_masked_embeddings = embeddings[:,:,masked_indices] # N x D x K # .detach().clone()\n",
        "        if self.verbose:\n",
        "            print('true masked embeddings shape', true_masked_embeddings.shape)\n",
        "\n",
        "        learned_embeddings_replace = embeddings.clone() # if not clone backward pass will complain as inplace modification not allowed\n",
        "        for i in range(len(masked_indices)):\n",
        "            learned_embeddings_replace[:,:,i] = masked_vector_learned_embedding\n",
        "        if self.verbose:\n",
        "            print('masked embeddings shape', embeddings.shape)\n",
        "\n",
        "        # feed masked samples to context encoder. Every timestep has an output\n",
        "        context_encoder_outputs = model.context_encoder(learned_embeddings_replace) # N x D x K\n",
        "        if self.verbose:\n",
        "            print('context encoder outputs shape', context_encoder_outputs.shape)\n",
        "\n",
        "        # context encoder_outputs of the masked input\n",
        "        predicted_masked_latent = context_encoder_outputs[:,:,masked_indices] # N x D x K\n",
        "        if self.verbose:\n",
        "            print('predicted context encoder outputs shape', predicted_masked_latent.shape)\n",
        "        return predicted_masked_latent, true_masked_embeddings\n",
        "\n",
        "    def loss(self, predictions, masked_latents):\n",
        "        '''\n",
        "        Follow implementation in https://github.com/dhruvbird/ml-notebooks/blob/main/nt-xent-loss/NT-Xent%20Loss.ipynb\n",
        "        @parameter\n",
        "            predictions:         (N x D x K) Batch-size embeddings of the model's guess for masked inputs\n",
        "            masked_latents:      (N x D x K) Batch-size embeddings of the feature encoder output of masked inputs\n",
        "\n",
        "        @return\n",
        "            batched mean contrastive loss\n",
        "        '''\n",
        "        losses = torch.zeros((masked_latents.shape[-1],), device=self.device)\n",
        "        # contrastive learning is computed one masked sample at a time\n",
        "        for k in range(masked_latents.shape[-1]):\n",
        "            predicted_masked_latent = predictions[:,k] # N x D\n",
        "            if self.verbose:\n",
        "                print('predicted masked latent shape', predicted_masked_latent.shape)\n",
        "            cos_sim = F.cosine_similarity(torch.unsqueeze(predicted_masked_latent, dim=-1), masked_latents, dim=1) # N x K\n",
        "            if self.verbose:\n",
        "                print('cosine similarity shape', cos_sim.shape)\n",
        "            labels = torch.zeros([cos_sim.shape[0], cos_sim.shape[1]], device=self.device) # N x K\n",
        "            labels[:,k] = 1\n",
        "            # print('labels', labels)\n",
        "            # losses.append(F.cross_entropy(cos_sim, labels, reduction='mean'))\n",
        "            losses[k] = F.cross_entropy(cos_sim, labels, reduction='mean')\n",
        "        if self.verbose:\n",
        "            print('losses', losses)\n",
        "        # return torch.mean(torch.tensor(losses))\n",
        "        return torch.mean(losses)\n",
        "\n",
        "    def train(self, model, train_params={}):\n",
        "        print('Training on ', self.device)\n",
        "        self.train_params.update(train_params)\n",
        "        num_epochs = self.train_params['num_epochs']\n",
        "        batch_size = self.train_params['batch_size']\n",
        "        print_every = self.train_params['print_every']\n",
        "\n",
        "        optimizer  = torch.optim.Adam(model.parameters())\n",
        "        dataloader_train = DataLoader(self.dataset_train, batch_size = batch_size, shuffle = True)\n",
        "        model.to(device=self.device)\n",
        "        model.train()\n",
        "        for e in range(num_epochs):\n",
        "            for t, (samples, _) in enumerate(dataloader_train):\n",
        "                samples = samples.to(device=self.device, dtype=torch.float32)\n",
        "                predictions, masked_latents = self.forward(model, samples)\n",
        "                loss = self.loss(predictions, masked_latents)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if t % print_every == 0:\n",
        "                    # writer.add_scalar(\"Loss/train\", loss.item(), e*len(dataloader)+t)\n",
        "                    print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "\n",
        "                metrics = {\"train/train_loss\": loss.item()}\n",
        "\n",
        "                del samples\n",
        "                del predictions\n",
        "                del masked_latents\n",
        "                del loss\n",
        "\n",
        "            eval_train_score, eval_test_score = self.finetune_eval_score(model)\n",
        "\n",
        "    def finetune_eval_score(self, model):\n",
        "        model.eval()\n",
        "        generator = torch.Generator().manual_seed(42)\n",
        "        val_train, val_test = random_split(self.dataset_val, [0.7, 0.3], generator=generator)\n",
        "        val_train_dataloader = DataLoader(val_train, batch_size = len(val_train), shuffle = True)\n",
        "        val_test_dataloader = DataLoader(val_test, batch_size = len(val_test), shuffle = True)\n",
        "\n",
        "        samples, labels = next(iter(val_train_dataloader))\n",
        "        samples = samples.to(device=self.device, dtype=torch.float32)\n",
        "        predictions = model(samples)\n",
        "        # print(predictions)\n",
        "        embeddings = torch.mean(predictions, dim=-1) # TODO is averaging the best strategy here, for classification?\n",
        "        # print(embeddings)\n",
        "        clf = LinearDiscriminantAnalysis()\n",
        "        clf.fit(embeddings.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
        "        train_score = clf.score(embeddings.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
        "        print('Eval train score:', train_score)\n",
        "\n",
        "        samples_test, labels_test = next(iter(val_test_dataloader))\n",
        "        samples_test = samples_test.to(device=self.device, dtype=torch.float32)\n",
        "        predictions = model(samples_test)\n",
        "        embeddings = torch.mean(predictions, dim=-1) # TODO is averaging the best strategy here, for classification?\n",
        "        test_score = clf.score(embeddings.detach().cpu().numpy(), labels_test.detach().cpu().numpy())\n",
        "        print('Eval test score:', test_score)\n",
        "        return train_score, test_score"
      ],
      "metadata": {
        "id": "2CN8FJDXaWYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import os\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import itertools\n",
        "\n",
        "class RelativePositioningTask():\n",
        "    def __init__(self,\n",
        "                dataset: torch.utils.data.Dataset,\n",
        "                win_length = 50,\n",
        "                tau_pos = 150,\n",
        "                tau_neg = 170,\n",
        "                n_samples = 1,\n",
        "                task_params={\n",
        "                    'mask_prob': 0.5\n",
        "                },\n",
        "                train_params={\n",
        "                    'num_epochs': 100,\n",
        "                    'batch_size': 10,\n",
        "                    'print_every': 10\n",
        "                },\n",
        "                verbose=False\n",
        "        ):\n",
        "        self.dataset = dataset\n",
        "        self.train_test_split()\n",
        "        self.win = win_length\n",
        "        self.tau_pos = tau_pos\n",
        "        self.tau_neg = tau_neg\n",
        "        self.n_samples = n_samples\n",
        "\n",
        "        self.train_params = train_params\n",
        "        self.mask_probability = task_params['mask_prob']\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.verbose=verbose\n",
        "        self.linear_ff = nn.Linear(768*11, 200)\n",
        "        self.loss_linear = nn.Linear(200, 1)\n",
        "    def train_test_split(self):\n",
        "        generator = torch.Generator().manual_seed(42)\n",
        "        self.dataset_train, self.dataset_val = torch.utils.data.random_split(self.dataset, [0.7,0.3], generator=generator)\n",
        "\n",
        "    def gRP(self, embeddings):\n",
        "        differences = []\n",
        "\n",
        "        for i in range(len(embeddings)):\n",
        "          differences.append(torch.abs(embeddings[i][0] - embeddings[i][1]))\n",
        "\n",
        "        return torch.stack(differences)\n",
        "\n",
        "    def forward(self, model, x):\n",
        "        samples = []\n",
        "        labels = []\n",
        "\n",
        "        for anchor_start in np.arange(0, x.shape[2]-self.win, self.win): # non-overlapping anchor window\n",
        "            # Positive window start t_pos:\n",
        "            #     - |t_pos - t_anchor| <= tau_pos\n",
        "            #           <-> t_pos <= tau_pos + t_anchor\n",
        "            #           <-> t_pos => t_anchor - tau_pos\n",
        "            #     - t_pos < T - win\n",
        "            #.    - t_pos > 0\n",
        "            pos_winds_start = np.arange(np.maximum(0, anchor_start - self.tau_pos), np.minimum(anchor_start+self.tau_pos, x.shape[2]-self.win), self.win) # valid positive samples onsets\n",
        "            if len(pos_winds_start) > 0:\n",
        "                # positive context\n",
        "                pos_winds = [x[:, :, sample_start:sample_start+self.win] for sample_start in np.random.choice(pos_winds_start, self.n_samples, replace=False)]\n",
        "                anchors = [x[:, :,anchor_start:anchor_start+self.win] for i in range(len(pos_winds))] # repeat same anchor window\n",
        "\n",
        "                anch = torch.stack([anchors[i].clone().detach() for i in range(len(anchors))])[0]\n",
        "                pos_w = torch.stack([pos_winds[i].clone().detach() for i in range(len(anchors))])[0]\n",
        "\n",
        "                samples.append(torch.stack([anch, pos_w])) # if anchors[i].shape == pos_winds[i].shape])\n",
        "                labels.append(torch.ones(len(anchors)))\n",
        "\n",
        "                # negative context\n",
        "                # Negative window start t_neg:\n",
        "                #     - |t_neg - t_anchor| > tau_neg\n",
        "                #           <-> t_neg > tau_neg + t_anchor\n",
        "                #           <-> t_neg < t_anchor - tau_neg\n",
        "                #     - t_neg < T - win\n",
        "                #.    - t_neg > 0\n",
        "                neg_winds_start = np.concatenate((np.arange(0, anchor_start-self.tau_neg, self.win), np.arange(anchor_start+self.tau_neg, x.shape[2]-self.win, self.win)))\n",
        "                neg_winds = [x[:, :,sample_start:sample_start+self.win] for sample_start in np.random.choice(neg_winds_start, self.n_samples, replace=False)]\n",
        "\n",
        "                anch = torch.stack([anchors[i].clone().detach() for i in range(len(anchors))])[0]\n",
        "                neg_w = torch.stack([neg_winds[i].clone().detach() for i in range(len(anchors))])[0]\n",
        "\n",
        "                samples.append(torch.stack([anch, neg_w])) # if anchors[i].shape == neg_winds[i].shape])\n",
        "                labels.append(torch.zeros(len(anchors)))\n",
        "\n",
        "        samples = torch.stack(samples) # N x 2 (anchors, pos/neg) x C x W\n",
        "        if len(samples) != len(labels):\n",
        "            raise ValueError('Number of samples and labels mismatch')\n",
        "        labels = torch.stack(labels)\n",
        "\n",
        "        embeddings = []\n",
        "\n",
        "        for i in range(samples.shape[0]):\n",
        "          embeddings.append(self.linear_ff(torch.flatten(model.feature_encoder(samples[i][:, 0]), start_dim = 1)))\n",
        "\n",
        "        differences = self.gRP(embeddings)\n",
        "        labels = labels.long()\n",
        "\n",
        "        return differences, labels\n",
        "\n",
        "    def loss(self, differences, labels):\n",
        "        linear_combination = self.loss_linear(differences)\n",
        "        # Calculate the loss\n",
        "        loss = torch.log(1 + torch.exp(-labels * linear_combination))\n",
        "        return loss.mean()\n",
        "\n",
        "    def train(self, model, train_params={}):\n",
        "        print('Training on ', self.device)\n",
        "        self.train_params.update(train_params)\n",
        "        num_epochs = self.train_params['num_epochs']\n",
        "        batch_size = self.train_params['batch_size']\n",
        "        print_every = self.train_params['print_every']\n",
        "\n",
        "        optimizer  = torch.optim.Adam(list(model.parameters()) + list(self.loss_linear.parameters()) + list(self.linear_ff.parameters()))\n",
        "        dataloader_train = DataLoader(self.dataset_train, batch_size = batch_size, shuffle = True)\n",
        "        model.to(device=self.device)\n",
        "        model.train()\n",
        "        for e in range(num_epochs):\n",
        "            for t, (samples, _) in enumerate(dataloader_train):\n",
        "                samples = samples.to(device=self.device, dtype=torch.float32)\n",
        "                differences, labels = self.forward(model, samples)\n",
        "                loss = self.loss(differences, labels)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if t % print_every == 0:\n",
        "                    # writer.add_scalar(\"Loss/train\", loss.item(), e*len(dataloader)+t)\n",
        "                    print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "\n",
        "                metrics = {\"train/train_loss\": loss.item()}\n",
        "\n",
        "                del samples\n",
        "                del differences\n",
        "                del labels\n",
        "                del loss\n",
        "\n",
        "            eval_train_score, eval_test_score = self.finetune_eval_score(model)\n",
        "\n",
        "    def finetune_eval_score(self, model):\n",
        "        model.eval()\n",
        "        generator = torch.Generator().manual_seed(42)\n",
        "        val_train, val_test = random_split(self.dataset_val, [0.7, 0.3], generator=generator)\n",
        "        val_train_dataloader = DataLoader(val_train, batch_size = len(val_train), shuffle = True)\n",
        "        val_test_dataloader = DataLoader(val_test, batch_size = len(val_test), shuffle = True)\n",
        "\n",
        "        samples, labels = next(iter(val_train_dataloader))\n",
        "        samples = samples.to(device=self.device, dtype=torch.float32)\n",
        "        predictions = model(samples)\n",
        "        # print(predictions)\n",
        "        embeddings = torch.mean(predictions, dim=-1) # TODO is averaging the best strategy here, for classification?\n",
        "        # print(embeddings)\n",
        "        clf = LinearDiscriminantAnalysis()\n",
        "        clf.fit(embeddings.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
        "        train_score = clf.score(embeddings.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
        "        print('Eval train score:', train_score)\n",
        "\n",
        "        samples_test, labels_test = next(iter(val_test_dataloader))\n",
        "        samples_test = samples_test.to(device=self.device, dtype=torch.float32)\n",
        "        predictions = model(samples_test)\n",
        "        embeddings = torch.mean(predictions, dim=-1) # TODO is averaging the best strategy here, for classification?\n",
        "        test_score = clf.score(embeddings.detach().cpu().numpy(), labels_test.detach().cpu().numpy())\n",
        "        print('Eval test score:', test_score)\n",
        "        return train_score, test_score"
      ],
      "metadata": {
        "id": "igxB4OepNZyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch\n",
        "\n",
        "class RandomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x = 10\n",
        "  def __len__(self):\n",
        "    return 100\n",
        "  def __getitem__(self, idx):\n",
        "    return torch.randn(128, 512), 1"
      ],
      "metadata": {
        "id": "4OyXZb4Vc20d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ssl_model import Wav2Vec2\n",
        "from ssl_utils import MaskedContrastiveLearningTask, RelativePositioningTask, TemporalShufflingTask\n",
        "\n",
        "data = RandomDataset()\n",
        "task = MaskedContrastiveLearningTask(data)\n",
        "model = Wav2Vec2([10, 3, 3, 3, 3, 2, 2], [2, 1, 1, 1, 1, 1, 1])\n",
        "task.train(model, {'num_epochs': 10, 'batch_size': 1, 'print_every': 1})"
      ],
      "metadata": {
        "id": "k8IXeNSid9eE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "outputId": "9b1ea253-030f-46a8-9df2-f4c6042a0e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on  cpu\n",
            "Epoch 0, Iteration 0, loss = 4.7965\n",
            "Epoch 0, Iteration 1, loss = 4.7960\n",
            "Epoch 0, Iteration 2, loss = 4.7981\n",
            "Epoch 0, Iteration 3, loss = 4.7957\n",
            "Epoch 0, Iteration 4, loss = 4.7960\n",
            "Epoch 0, Iteration 5, loss = 4.7948\n",
            "Epoch 0, Iteration 6, loss = 4.7953\n",
            "Epoch 0, Iteration 7, loss = 4.7959\n",
            "Epoch 0, Iteration 8, loss = 4.7956\n",
            "Epoch 0, Iteration 9, loss = 4.7954\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2e6323709f36>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaskedContrastiveLearningTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWav2Vec2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'print_every'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/ssl_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_params)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_latents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import os\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "class TemporalShufflingTask():\n",
        "    def __init__(self,\n",
        "                dataset: torch.utils.data.Dataset,\n",
        "                win_length = 50,\n",
        "                tau_pos = 150,\n",
        "                tau_neg = 151,\n",
        "                n_samples = 1,\n",
        "                stride = 1,\n",
        "                task_params={\n",
        "                    'mask_prob': 0.5\n",
        "                },\n",
        "                train_params={\n",
        "                    'num_epochs': 100,\n",
        "                    'batch_size': 10,\n",
        "                    'print_every': 10\n",
        "                },\n",
        "                verbose=False\n",
        "        ):\n",
        "        self.dataset = dataset\n",
        "        self.train_test_split()\n",
        "        self.win = win_length\n",
        "        self.tau_pos = tau_pos\n",
        "        self.tau_neg = tau_neg\n",
        "        self.n_samples = n_samples\n",
        "        self.stride = stride\n",
        "\n",
        "        self.train_params = train_params\n",
        "        self.mask_probability = task_params['mask_prob']\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.verbose=verbose\n",
        "        self.linear_ff = nn.Linear(768*11, 200)\n",
        "        self.loss_linear = nn.Linear(400, 1)\n",
        "\n",
        "    def train_test_split(self):\n",
        "        generator = torch.Generator().manual_seed(42)\n",
        "        self.dataset_train, self.dataset_val = torch.utils.data.random_split(self.dataset, [0.7,0.3], generator=generator)\n",
        "\n",
        "    def gTS(self, embeddings):\n",
        "        differences = []\n",
        "\n",
        "        for i in range(len(embeddings)):\n",
        "          differences.append(torch.cat((torch.abs(embeddings[i][0] - embeddings[i][1]), torch.abs(embeddings[i][1] - embeddings[i][2])), dim = 0))\n",
        "\n",
        "        return torch.stack(differences)\n",
        "\n",
        "    def forward(self, model, x):\n",
        "        samples = []\n",
        "        labels = []\n",
        "\n",
        "        tau_pos = self.tau_pos\n",
        "        for pos_start in np.arange(0, x.shape[2], tau_pos): # non-overlapping positive contexts\n",
        "            if pos_start + tau_pos < x.shape[2]:\n",
        "                pos_winds = [x[:, :, pos_start:pos_start+self.win], x[:, :, pos_start+self.win*2:pos_start+self.win*3]] # two positive windows\\\n",
        "                inorder = torch.stack(pos_winds[:1] + [x[:, :, pos_start+self.win:pos_start+self.win*2]] + pos_winds[1:])\n",
        "                samples.extend([inorder, torch.flip(inorder, dims = [0])])\n",
        "                labels.extend(torch.ones(2))\n",
        "\n",
        "                # for negative windows, want both sides of anchor window\n",
        "                neg_winds_start = np.concatenate((np.arange(0, pos_start-self.tau_neg-self.win, self.stride), np.arange(pos_start+tau_pos+self.tau_neg, x.shape[2]-self.win, self.stride)))\n",
        "                selected_neg_start = np.random.choice(neg_winds_start, 1, replace=False)[0]\n",
        "                disorder = torch.stack(pos_winds[:1] + [x[:,:,selected_neg_start:selected_neg_start+self.win]] + pos_winds[1:]) # two positive windows, disorder sample added to the end\n",
        "                samples.extend([disorder, torch.flip(disorder, dims = [0])])\n",
        "                labels.extend(torch.zeros(2))\n",
        "\n",
        "        samples = torch.stack(samples)\n",
        "        labels = torch.stack(labels).unsqueeze(1)\n",
        "        if len(samples) != len(labels):\n",
        "            raise ValueError('Number of samples and labels mismatch')\n",
        "\n",
        "        embeddings = []\n",
        "\n",
        "        for i in range(samples.shape[0]):\n",
        "          embeddings.append(self.linear_ff(torch.flatten(model.feature_encoder(samples[i][:, 0]), start_dim = 1)))\n",
        "\n",
        "        differences = self.gTS(embeddings)\n",
        "        labels = labels.long()\n",
        "\n",
        "        return differences, labels\n",
        "\n",
        "    def loss(self, differences, labels):\n",
        "        linear_combination = self.loss_linear(differences)\n",
        "        # Calculate the loss\n",
        "        loss = torch.log(1 + torch.exp(-labels * linear_combination))\n",
        "        return loss.mean()\n",
        "\n",
        "    def train(self, model, train_params={}):\n",
        "        print('Training on ', self.device)\n",
        "        self.train_params.update(train_params)\n",
        "        num_epochs = self.train_params['num_epochs']\n",
        "        batch_size = self.train_params['batch_size']\n",
        "        print_every = self.train_params['print_every']\n",
        "\n",
        "        optimizer  = torch.optim.Adam(list(model.parameters()) + list(self.loss_linear.parameters()) + list(self.linear_ff.parameters()))\n",
        "        dataloader_train = DataLoader(self.dataset_train, batch_size = batch_size, shuffle = True)\n",
        "        model.to(device=self.device)\n",
        "        model.train()\n",
        "        for e in range(num_epochs):\n",
        "            for t, (samples, _) in enumerate(dataloader_train):\n",
        "                samples = samples.to(device=self.device, dtype=torch.float32)\n",
        "                differences, labels = self.forward(model, samples)\n",
        "                loss = self.loss(differences, labels)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if t % print_every == 0:\n",
        "                    # writer.add_scalar(\"Loss/train\", loss.item(), e*len(dataloader)+t)\n",
        "                    print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "\n",
        "                metrics = {\"train/train_loss\": loss.item()}\n",
        "\n",
        "                del samples\n",
        "                del differences\n",
        "                del labels\n",
        "                del loss\n",
        "\n",
        "            eval_train_score, eval_test_score = self.finetune_eval_score(model)\n",
        "\n",
        "    def finetune_eval_score(self, model):\n",
        "        model.eval()\n",
        "        generator = torch.Generator().manual_seed(42)\n",
        "        val_train, val_test = random_split(self.dataset_val, [0.7, 0.3], generator=generator)\n",
        "        val_train_dataloader = DataLoader(val_train, batch_size = len(val_train), shuffle = True)\n",
        "        val_test_dataloader = DataLoader(val_test, batch_size = len(val_test), shuffle = True)\n",
        "\n",
        "        samples, labels = next(iter(val_train_dataloader))\n",
        "        samples = samples.to(device=self.device, dtype=torch.float32)\n",
        "        predictions = model(samples)\n",
        "        # print(predictions)\n",
        "        embeddings = torch.mean(predictions, dim=-1) # TODO is averaging the best strategy here, for classification?\n",
        "        # print(embeddings)\n",
        "        clf = LinearDiscriminantAnalysis()\n",
        "        clf.fit(embeddings.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
        "        train_score = clf.score(embeddings.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
        "        print('Eval train score:', train_score)\n",
        "\n",
        "        samples_test, labels_test = next(iter(val_test_dataloader))\n",
        "        samples_test = samples_test.to(device=self.device, dtype=torch.float32)\n",
        "        predictions = model(samples_test)\n",
        "        embeddings = torch.mean(predictions, dim=-1) # TODO is averaging the best strategy here, for classification?\n",
        "        test_score = clf.score(embeddings.detach().cpu().numpy(), labels_test.detach().cpu().numpy())\n",
        "        print('Eval test score:', test_score)\n",
        "        return train_score, test_score"
      ],
      "metadata": {
        "id": "Dx7W4KSiGjdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import os\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "class CPC():\n",
        "    def __init__(self,\n",
        "                dataset: torch.utils.data.Dataset,\n",
        "                win_length = 50,\n",
        "                tau_pos = 150,\n",
        "                tau_neg = 151,\n",
        "                n_samples = 1,\n",
        "                stride = 1,\n",
        "                task_params={\n",
        "                    'mask_prob': 0.5\n",
        "                },\n",
        "                train_params={\n",
        "                    'num_epochs': 100,\n",
        "                    'batch_size': 10,\n",
        "                    'print_every': 10\n",
        "                },\n",
        "                verbose=False\n",
        "        ):\n",
        "        self.dataset = dataset\n",
        "        self.train_test_split()\n",
        "        self.win = win_length\n",
        "        self.tau_pos = tau_pos\n",
        "        self.tau_neg = tau_neg\n",
        "        self.n_samples = n_samples\n",
        "        self.stride = stride\n",
        "        self.Nc = 5\n",
        "        self.Np = 2\n",
        "\n",
        "        self.train_params = train_params\n",
        "        self.mask_probability = task_params['mask_prob']\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.verbose=verbose\n",
        "\n",
        "    def train_test_split(self):\n",
        "        generator = torch.Generator().manual_seed(42)\n",
        "        self.dataset_train, self.dataset_val = torch.utils.data.random_split(self.dataset, [0.7,0.3], generator=generator)\n",
        "\n",
        "    def forward(self, model, x):\n",
        "        context_windows = []\n",
        "        future_windows = []\n",
        "        negative_windows = []\n",
        "        for ti in np.arange(0, x.shape[2]-self.win*(self.Nc+self.Np), self.win*self.Nc):\n",
        "            # ti is the index of the first window in the context array\n",
        "            for st in np.arange(ti, ti+(self.win*self.Nc), self.win):\n",
        "              context_windows.append(x[:, :, st:st+self.win])\n",
        "            for st in np.arange(ti+(self.win*self.Nc), ti+(self.win*(self.Nc+self.Np)), self.win):\n",
        "              future_windows.append(x[:, :, st:st+self.win])\n",
        "\n",
        "        negative_windows = []\n",
        "        for i in range(len(future_windows)):\n",
        "            negative_windows.append([np.random.choice(arr, replace=False) for arr in future_windows[:i] + future_windows[i+1:]])\n",
        "\n",
        "        return list(zip(context_windows, future_windows, negative_windows))\n",
        "\n",
        "        embeddings = []\n",
        "\n",
        "        for i in range(samples.shape[0]):\n",
        "          embeddings.append(model.feature_encoder(samples[i][:, 0]))\n",
        "\n",
        "        predictions = []\n",
        "\n",
        "        for i in range(len(embeddings)):\n",
        "          predictions.append(model.classify(embeddings[i]))\n",
        "\n",
        "        predictions = torch.stack(predictions)\n",
        "        labels = labels.long()\n",
        "\n",
        "        return predictions, labels\n",
        "\n",
        "    def loss(self, predictions, labels):\n",
        "        return F.cross_entropy(predictions, labels)\n",
        "\n",
        "    def train(self, model, train_params={}):\n",
        "        print('Training on ', self.device)\n",
        "        self.train_params.update(train_params)\n",
        "        num_epochs = self.train_params['num_epochs']\n",
        "        batch_size = self.train_params['batch_size']\n",
        "        print_every = self.train_params['print_every']\n",
        "\n",
        "        optimizer  = torch.optim.Adam(model.parameters())\n",
        "        dataloader_train = DataLoader(self.dataset_train, batch_size = batch_size, shuffle = True)\n",
        "        model.to(device=self.device)\n",
        "        model.train()\n",
        "        for e in range(num_epochs):\n",
        "            for t, (samples, _) in enumerate(dataloader_train):\n",
        "                samples = samples.to(device=self.device, dtype=torch.float32)\n",
        "                predictions, labels = self.forward(model, samples)\n",
        "                loss = self.loss(predictions, labels)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if t % print_every == 0:\n",
        "                    # writer.add_scalar(\"Loss/train\", loss.item(), e*len(dataloader)+t)\n",
        "                    print('Epoch %d, Iteration %d, loss = %.4f' % (e, t, loss.item()))\n",
        "\n",
        "                metrics = {\"train/train_loss\": loss.item()}\n",
        "\n",
        "                del samples\n",
        "                del predictions\n",
        "                del labels\n",
        "                del loss\n",
        "\n",
        "            eval_train_score, eval_test_score = self.finetune_eval_score(model)\n",
        "\n",
        "    def finetune_eval_score(self, model):\n",
        "        model.eval()\n",
        "        generator = torch.Generator().manual_seed(42)\n",
        "        val_train, val_test = random_split(self.dataset_val, [0.7, 0.3], generator=generator)\n",
        "        val_train_dataloader = DataLoader(val_train, batch_size = len(val_train), shuffle = True)\n",
        "        val_test_dataloader = DataLoader(val_test, batch_size = len(val_test), shuffle = True)\n",
        "\n",
        "        samples, labels = next(iter(val_train_dataloader))\n",
        "        samples = samples.to(device=self.device, dtype=torch.float32)\n",
        "        predictions = model(samples)\n",
        "        # print(predictions)\n",
        "        embeddings = torch.mean(predictions, dim=-1) # TODO is averaging the best strategy here, for classification?\n",
        "        # print(embeddings)\n",
        "        clf = LinearDiscriminantAnalysis()\n",
        "        clf.fit(embeddings.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
        "        train_score = clf.score(embeddings.detach().cpu().numpy(), labels.detach().cpu().numpy())\n",
        "        print('Eval train score:', train_score)\n",
        "\n",
        "        samples_test, labels_test = next(iter(val_test_dataloader))\n",
        "        samples_test = samples_test.to(device=self.device, dtype=torch.float32)\n",
        "        predictions = model(samples_test)\n",
        "        embeddings = torch.mean(predictions, dim=-1) # TODO is averaging the best strategy here, for classification?\n",
        "        test_score = clf.score(embeddings.detach().cpu().numpy(), labels_test.detach().cpu().numpy())\n",
        "        print('Eval test score:', test_score)\n",
        "        return train_score, test_score"
      ],
      "metadata": {
        "id": "iaAi62suAqrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(TransformerLayer, self).__init__()\n",
        "    self.pos_emb = PosEmb()\n",
        "    self.norm = nn.Sequential(\n",
        "        nn.LayerNorm((768,), eps=1e-05, elementwise_affine=True),\n",
        "        nn.Dropout(p=0.1, inplace=False)\n",
        "    )\n",
        "    self.attention = []\n",
        "    for i in range(12):\n",
        "      self.attention.append(nn.MultiheadAttention(768, 1))\n",
        "    self.feed_forward = nn.Sequential(\n",
        "        nn.Dropout(p=0.1, inplace=False),\n",
        "        nn.Linear(in_features=768, out_features=3072, bias=True),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(in_features=3072, out_features=768, bias=True),\n",
        "        nn.Dropout(p=0.1, inplace=False),\n",
        "        nn.LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "  def forward(self, x):\n",
        "    x = self.pos_emb(x) + torch.permute(x, (0,2,1))\n",
        "    x = self.norm(x)\n",
        "    for i in range(12):\n",
        "      x = self.attention[i](x, x, x)[0]\n",
        "    return torch.permute(self.feed_forward(x), (0,2,1))"
      ],
      "metadata": {
        "id": "1fAciFTkGp2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = RandomDataset()\n",
        "task = CPC(data)\n",
        "model = FeatureEncoder([10, 3, 3, 3, 3, 2, 2], [2, 1, 1, 1, 1, 1, 1], 2)\n",
        "task.train(model, {'num_epochs': 10, 'batch_size': 1, 'print_every': 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "7BoMZhrsTjcA",
        "outputId": "2376cdf1-5e11-43cd-b8c2-2ffd95cb772c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on  cpu\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "low >= high",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-9fbe90d72361>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCPC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'print_every'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-6b856a2ccc50>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_params)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-6b856a2ccc50>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mti\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mcombined_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mti\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNc\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0mlow_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_bounded_integers.pyx\u001b[0m in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: low >= high"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_conv = FeatureEncoder([10, 3, 3, 3, 3, 2, 2], [2, 1, 1, 1, 1, 1, 1])\n",
        "print(model_conv.receptive_field())\n",
        "model_encoder = TransformerLayer()\n",
        "print(model_encoder)"
      ],
      "metadata": {
        "id": "H4OhtxTM-YfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b20ad898-006e-45e8-9ecf-35db306ef945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerLayer(\n",
            "  (pos_emb): PosEmb(\n",
            "    (conv): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
            "    (activation): GELU(approximate='none')\n",
            "  )\n",
            "  (norm): Sequential(\n",
            "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (feed_forward): Sequential(\n",
            "    (0): Dropout(p=0.1, inplace=False)\n",
            "    (1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (2): GELU(approximate='none')\n",
            "    (3): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (4): Dropout(p=0.1, inplace=False)\n",
            "    (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(y, z):\n",
        "  mat = torch.einsum('bdj,bdk->bjk', y, z)\n",
        "  N = y.shape[2]\n",
        "  target = torch.arange(N)\n",
        "  loss = 0\n",
        "  for i in range(y.shape[0]):\n",
        "    loss += torch.nn.functional.cross_entropy(mat[0], target)\n",
        "  return loss/y.shape[0]"
      ],
      "metadata": {
        "id": "KwCli3awyZZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "x = torch.randn(8, 128, 512)\n",
        "y = model_conv(x)\n",
        "print(y.shape)\n",
        "z = model_encoder(torch.randn(8, 768, 720))\n",
        "print(z.shape)\n",
        "# y1 = y\n",
        "# rlist = []\n",
        "# for i in range(16):\n",
        "#   rlist.append(random.randint(1, 112))\n",
        "# for r in rlist:\n",
        "#   y1[:, :, r] = 0\n",
        "# z = model_encoder(y1)\n",
        "# z1 = z[:, :, rlist]\n",
        "# y2 = y[:, :, rlist]\n",
        "# print(contrastive_loss(y2, z1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cwS_NKf9eRA",
        "outputId": "9eadfaff-d4a3-47f5-f278-da2d27959a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 768, 242])\n",
            "torch.Size([8, 768, 720])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_encoder(y).shape"
      ],
      "metadata": {
        "id": "cfnZaTr84ARc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff56a33c-bfc5-43e3-9cfc-5faff647facc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 768, 114])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class KrakenEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(KrakenEncoder, self).__init__()\n",
        "    self.conv0 = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size = (4, 2), stride = (4, 2)),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.GroupNorm(1,64)\n",
        "    )\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(64, 128, kernel_size = (4, 2)),\n",
        "        nn.LeakyReLU(),\n",
        "        nn.GroupNorm(1,128)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.MaxPool2d((4, 2), stride = (1, 2)),\n",
        "        nn.Conv2d(128, 256, kernel_size = (3, 3)),\n",
        "        nn.MaxPool2d((4, 2), stride = (1, 2))\n",
        "    )\n",
        "    self.bilstm = nn.LSTM(10*256, 256, 3, bidirectional= True, batch_first=True)\n",
        "  def forward(self, x):\n",
        "    x = self.conv0(x)\n",
        "    x = self.conv1(x)\n",
        "    x = self.conv2(x)\n",
        "    print(x.shape)\n",
        "    x = torch.transpose(x,2,3)\n",
        "    x = torch.flatten(x, start_dim = 1, end_dim = 2)\n",
        "    x = torch.transpose(x,1,2)\n",
        "    print('input dim before LSTM', x.shape)\n",
        "    x = self.bilstm(x)\n",
        "    return torch.transpose(x[0],1,2)"
      ],
      "metadata": {
        "id": "ajIU4OfxVqTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = KrakenEncoder()\n",
        "x = torch.randn(8, 1, 1024, 96) # why 1?\n",
        "print(encoder(x).shape) # 256 is sequence length which is wrong. It's the channel/output dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhze1wyKMATh",
        "outputId": "49e7b32b-8283-410f-d02c-110386d174d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 256, 245, 10])\n",
            "input dim before LSTM torch.Size([8, 245, 2560])\n",
            "torch.Size([8, 512, 245])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = KrakenEncoder()\n",
        "pytorch_total_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
        "print(pytorch_total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPw_v3EOGvfk",
        "outputId": "abd10131-eeb7-4a3f-ca2b-aab011614c34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9286976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DwIUfybTUQlf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}