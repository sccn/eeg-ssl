{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6959f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def _generate_negatives(z):\n",
    "    \"\"\"Generate negative samples to compare each sequence location against\"\"\"\n",
    "    batch_size, feat, full_len = z.shape\n",
    "    z_k = z.permute([0, 2, 1]).reshape(-1, feat)\n",
    "    with torch.no_grad():\n",
    "        # candidates = torch.arange(full_len).unsqueeze(-1).expand(-1, self.num_negatives).flatten()\n",
    "        negative_inds = torch.randint(0, full_len-1, size=(batch_size, full_len * 20))\n",
    "        # From wav2vec 2.0 implementation, I don't understand\n",
    "        # negative_inds[negative_inds >= candidates] += 1\n",
    "\n",
    "        for i in range(1, batch_size):\n",
    "            negative_inds[i] += i * full_len\n",
    "\n",
    "    z_k = z_k[negative_inds.view(-1)].view(batch_size, full_len, 20, feat)\n",
    "    return z_k, negative_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00eb8057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdt-young112\u001b[0m (\u001b[33msccn\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dung/eeg-ssl/notebooks/wandb/run-20250509_092657-03vkhp7j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sccn/eeg-ssl-notebooks/runs/03vkhp7j' target=\"_blank\">drawn-star-10</a></strong> to <a href='https://wandb.ai/sccn/eeg-ssl-notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sccn/eeg-ssl-notebooks' target=\"_blank\">https://wandb.ai/sccn/eeg-ssl-notebooks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sccn/eeg-ssl-notebooks/runs/03vkhp7j' target=\"_blank\">https://wandb.ai/sccn/eeg-ssl-notebooks/runs/03vkhp7j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model-1t2pzp1g:v7, 443.31MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.6\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('sccn/eeg-ssl/model-1t2pzp1g:v7', type='model')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13dcd230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.1702, -1.4821, -1.6744],\n",
       "           [ 0.7960, -0.8282,  1.3160],\n",
       "           [-0.0633,  0.5059, -0.6821],\n",
       "           ...,\n",
       "           [ 1.1702, -1.4821, -1.6744],\n",
       "           [ 0.4705,  1.6255, -0.0271],\n",
       "           [ 0.5236,  1.1841, -1.6002]],\n",
       " \n",
       "          [[-2.8905,  1.5216,  1.0312],\n",
       "           [-0.1559,  0.0274,  0.2242],\n",
       "           [ 0.7960, -0.8282,  1.3160],\n",
       "           ...,\n",
       "           [-0.1080,  0.7950,  0.6234],\n",
       "           [-2.8905,  1.5216,  1.0312],\n",
       "           [ 0.2810,  0.9419,  0.5902]],\n",
       " \n",
       "          [[ 0.7960, -0.8282,  1.3160],\n",
       "           [-1.7643,  0.4037,  0.4514],\n",
       "           [-0.7473,  0.7631,  1.3274],\n",
       "           ...,\n",
       "           [ 0.4342, -1.3391, -1.1102],\n",
       "           [ 0.2810,  0.9419,  0.5902],\n",
       "           [-1.6061,  0.5028, -0.9767]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 0.0488,  0.9339, -0.7958],\n",
       "           [-0.1559,  0.0274,  0.2242],\n",
       "           [-0.1080,  0.7950,  0.6234],\n",
       "           ...,\n",
       "           [-0.7473,  0.7631,  1.3274],\n",
       "           [ 0.0488,  0.9339, -0.7958],\n",
       "           [-0.0633,  0.5059, -0.6821]],\n",
       " \n",
       "          [[ 0.7960, -0.8282,  1.3160],\n",
       "           [-0.0633,  0.5059, -0.6821],\n",
       "           [-1.7643,  0.4037,  0.4514],\n",
       "           ...,\n",
       "           [ 0.4342, -1.3391, -1.1102],\n",
       "           [ 0.0488,  0.9339, -0.7958],\n",
       "           [ 0.4342, -1.3391, -1.1102]],\n",
       " \n",
       "          [[-0.7519, -0.5191,  0.5263],\n",
       "           [-0.0633,  0.5059, -0.6821],\n",
       "           [ 1.3901,  0.9859,  1.7501],\n",
       "           ...,\n",
       "           [ 0.4705,  1.6255, -0.0271],\n",
       "           [ 0.4705,  1.6255, -0.0271],\n",
       "           [ 0.1325, -0.2393, -0.4523]]],\n",
       " \n",
       " \n",
       "         [[[-1.8121, -0.0424,  0.6629],\n",
       "           [-0.1902, -1.0264, -0.0935],\n",
       "           [-2.1086, -0.8907, -0.4486],\n",
       "           ...,\n",
       "           [ 0.3946,  0.6640, -0.2054],\n",
       "           [ 0.3282,  0.3748,  0.9800],\n",
       "           [ 0.7882, -0.9304,  0.2633]],\n",
       " \n",
       "          [[ 1.5605, -1.6718,  0.5206],\n",
       "           [ 0.3946,  0.6640, -0.2054],\n",
       "           [-0.1902, -1.0264, -0.0935],\n",
       "           ...,\n",
       "           [-0.0404, -0.6674, -0.7478],\n",
       "           [-2.1086, -0.8907, -0.4486],\n",
       "           [-0.4296,  0.6792, -0.6698]],\n",
       " \n",
       "          [[-0.1902, -1.0264, -0.0935],\n",
       "           [ 0.2186, -0.7437, -0.2828],\n",
       "           [ 0.3282,  0.3748,  0.9800],\n",
       "           ...,\n",
       "           [ 0.6751,  0.5636,  0.7736],\n",
       "           [ 0.3437, -0.6172,  0.0827],\n",
       "           [ 1.7615,  0.9842,  1.5119]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-1.8121, -0.0424,  0.6629],\n",
       "           [ 0.1324, -0.2181,  1.2129],\n",
       "           [ 0.3437, -0.6172,  0.0827],\n",
       "           ...,\n",
       "           [ 0.3946,  0.6640, -0.2054],\n",
       "           [-2.1086, -0.8907, -0.4486],\n",
       "           [ 0.2186, -0.7437, -0.2828]],\n",
       " \n",
       "          [[-1.8121, -0.0424,  0.6629],\n",
       "           [-2.1086, -0.8907, -0.4486],\n",
       "           [ 0.3946,  0.6640, -0.2054],\n",
       "           ...,\n",
       "           [-0.4296,  0.6792, -0.6698],\n",
       "           [ 0.3437, -0.6172,  0.0827],\n",
       "           [ 0.2263, -0.5983, -0.6068]],\n",
       " \n",
       "          [[ 1.7615,  0.9842,  1.5119],\n",
       "           [ 0.8971, -1.0044, -0.3311],\n",
       "           [ 1.7615,  0.9842,  1.5119],\n",
       "           ...,\n",
       "           [ 2.4455,  1.7595,  0.1698],\n",
       "           [-1.8121, -0.0424,  0.6629],\n",
       "           [ 0.7882, -0.9304,  0.2633]]]]),\n",
       " tensor([[ 3, 11,  1, 14, 17, 11, 18, 15, 17,  4,  9, 12,  6,  9, 12, 16,  4,  3,\n",
       "           9,  0, 13, 18, 11, 17, 15, 16,  3, 12,  6,  9,  9, 10, 10, 17, 13,  2,\n",
       "          16, 16, 13,  7, 11, 15,  4,  4,  1,  5, 17,  7, 17, 11,  9,  6,  7,  2,\n",
       "           4,  2, 12,  6,  7, 17, 13, 13, 11, 16,  9,  5, 18, 16,  6,  4, 17,  0,\n",
       "          11, 16,  5,  7,  7, 12,  8,  5,  7, 10,  3, 15, 14,  5, 12, 10,  8,  8,\n",
       "          11,  7,  1,  3, 13, 14,  4,  0,  7,  1,  0,  6,  2,  8, 11, 11, 17, 17,\n",
       "          11, 13,  8,  7,  4, 12,  9, 16, 12,  1,  7, 11,  8,  0,  1, 16,  2,  9,\n",
       "           5, 10, 17,  9, 16, 18,  1, 13, 12, 14, 13,  3,  3,  5,  4, 13, 15,  7,\n",
       "          12,  1,  9,  5, 15, 16,  6, 13,  5,  1,  6, 15,  5,  6,  5, 11, 18,  9,\n",
       "          11, 18,  9,  4, 17, 10,  7,  4, 16,  3, 10,  7, 13,  5,  3,  0,  2, 14,\n",
       "          10,  8,  8,  8,  0,  9, 11,  4,  2,  4, 13,  5,  2,  5,  5, 14, 12,  0,\n",
       "          18, 18, 18,  9, 10, 15, 11, 17,  7,  3, 13, 14, 18, 12, 12,  8,  2,  0,\n",
       "          17,  7, 14,  7,  8,  6, 11,  6, 16,  1,  2,  9,  0, 12, 13,  6,  2,  7,\n",
       "           0,  5,  8,  7,  7,  6, 15, 12,  2,  9,  3, 17,  0, 13,  5,  0, 11, 17,\n",
       "          12,  5,  0, 11, 12,  2, 11,  9,  7,  6, 18,  2,  6, 15, 12,  2,  8,  9,\n",
       "           5,  8,  3, 15, 18,  6,  0,  1,  5, 11, 16,  4, 17, 12, 17,  2,  2, 11,\n",
       "           9,  4,  5, 12,  4, 10, 16,  5,  3, 10,  2, 15, 16,  0,  4,  8,  1, 11,\n",
       "          18, 14, 13,  2,  0,  0,  1, 13, 13, 11, 14,  4, 13, 10,  5, 11, 17,  1,\n",
       "          11,  1, 16,  0, 11, 13,  1, 11,  0,  6,  7,  7,  7,  3, 17,  9, 10, 18,\n",
       "          16,  5,  6, 18,  3,  7,  2,  0, 14,  5,  5,  9, 12, 14, 15,  4, 10,  1,\n",
       "          11,  1, 15,  1, 11, 12,  3,  2, 18, 16, 13,  2,  5,  5, 13,  0, 10,  6,\n",
       "          10,  6, 12,  1,  8,  9,  7, 16, 13,  9,  4, 15, 10,  5, 14, 13, 16, 15,\n",
       "           7,  9,  9,  2],\n",
       "         [28, 37, 26, 35, 35, 25, 26, 33, 32, 29, 24, 24, 33, 20, 37, 23, 29, 20,\n",
       "          34, 23, 36, 20, 37, 32, 30, 21, 38, 21, 25, 34, 22, 36, 23, 25, 24, 29,\n",
       "          20, 29, 26, 25, 37, 33, 34, 33, 35, 20, 34, 34, 32, 22, 35, 33, 36, 28,\n",
       "          38, 31, 24, 31, 35, 21, 33, 25, 23, 26, 33, 21, 31, 28, 25, 35, 28, 37,\n",
       "          30, 27, 37, 21, 37, 29, 22, 32, 32, 28, 20, 20, 21, 33, 20, 30, 33, 32,\n",
       "          38, 25, 31, 34, 31, 38, 35, 24, 33, 37, 29, 30, 34, 22, 25, 33, 33, 22,\n",
       "          37, 20, 36, 35, 38, 36, 26, 34, 23, 31, 33, 38, 20, 35, 22, 25, 20, 25,\n",
       "          25, 32, 23, 25, 37, 32, 38, 26, 27, 33, 22, 34, 33, 28, 20, 24, 32, 30,\n",
       "          36, 23, 36, 21, 36, 33, 37, 32, 25, 23, 33, 20, 37, 23, 26, 36, 28, 30,\n",
       "          38, 38, 21, 23, 31, 37, 36, 20, 29, 29, 37, 22, 21, 32, 23, 32, 35, 35,\n",
       "          35, 31, 24, 36, 21, 25, 38, 26, 36, 32, 32, 27, 33, 24, 36, 35, 29, 34,\n",
       "          22, 20, 33, 32, 27, 34, 23, 33, 28, 20, 27, 22, 30, 31, 24, 38, 34, 37,\n",
       "          26, 33, 21, 22, 22, 20, 26, 34, 32, 38, 21, 33, 34, 28, 29, 30, 37, 35,\n",
       "          25, 21, 24, 20, 36, 30, 32, 34, 20, 20, 37, 38, 34, 22, 27, 32, 27, 36,\n",
       "          30, 32, 31, 28, 23, 27, 32, 35, 29, 20, 37, 28, 25, 22, 25, 25, 35, 21,\n",
       "          29, 36, 32, 33, 20, 25, 24, 30, 24, 23, 25, 29, 34, 23, 37, 29, 37, 24,\n",
       "          29, 27, 22, 27, 25, 31, 33, 26, 35, 21, 25, 27, 35, 28, 27, 24, 30, 36,\n",
       "          27, 22, 37, 21, 20, 25, 36, 23, 27, 26, 36, 25, 31, 36, 28, 36, 29, 20,\n",
       "          37, 33, 31, 31, 36, 37, 33, 30, 35, 24, 34, 28, 27, 27, 37, 25, 28, 38,\n",
       "          35, 30, 20, 32, 30, 23, 36, 28, 34, 30, 23, 29, 27, 21, 23, 20, 26, 33,\n",
       "          28, 26, 20, 21, 34, 30, 36, 20, 31, 23, 21, 22, 28, 22, 38, 38, 34, 25,\n",
       "          35, 24, 21, 22, 21, 28, 32, 34, 27, 30, 21, 29, 28, 36, 21, 27, 34, 21,\n",
       "          32, 30, 28, 23]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_generate_negatives(torch.randn(2, 3, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50a31516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================================================================\n",
      "Layer (type (var_name):depth-idx)             Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "=================================================================================================================================================\n",
      "Deep4Net (Deep4Net)                           [1, 128, 500]             [1, 2]                    --                        --\n",
      "├─Ensure4d (ensuredims): 1-1                  [1, 128, 500]             [1, 128, 500, 1]          --                        --\n",
      "├─Rearrange (dimshuffle): 1-2                 [1, 128, 500, 1]          [1, 1, 500, 128]          --                        --\n",
      "├─CombinedConv (conv_time_spat): 1-3          [1, 1, 500, 128]          [1, 25, 491, 1]           80,275                    --\n",
      "├─BatchNorm2d (bnorm): 1-4                    [1, 25, 491, 1]           [1, 25, 491, 1]           50                        --\n",
      "├─ELU (conv_nonlin): 1-5                      [1, 25, 491, 1]           [1, 25, 491, 1]           --                        --\n",
      "├─MaxPool2d (pool): 1-6                       [1, 25, 491, 1]           [1, 25, 163, 1]           --                        [3, 1]\n",
      "├─Identity (pool_nonlin): 1-7                 [1, 25, 163, 1]           [1, 25, 163, 1]           --                        --\n",
      "├─Dropout (drop_2): 1-8                       [1, 25, 163, 1]           [1, 25, 163, 1]           --                        --\n",
      "├─Conv2d (conv_2): 1-9                        [1, 25, 163, 1]           [1, 50, 154, 1]           12,500                    [10, 1]\n",
      "├─BatchNorm2d (bnorm_2): 1-10                 [1, 50, 154, 1]           [1, 50, 154, 1]           100                       --\n",
      "├─ELU (nonlin_2): 1-11                        [1, 50, 154, 1]           [1, 50, 154, 1]           --                        --\n",
      "├─MaxPool2d (pool_2): 1-12                    [1, 50, 154, 1]           [1, 50, 51, 1]            --                        [3, 1]\n",
      "├─Identity (pool_nonlin_2): 1-13              [1, 50, 51, 1]            [1, 50, 51, 1]            --                        --\n",
      "├─Dropout (drop_3): 1-14                      [1, 50, 51, 1]            [1, 50, 51, 1]            --                        --\n",
      "├─Conv2d (conv_3): 1-15                       [1, 50, 51, 1]            [1, 100, 42, 1]           50,000                    [10, 1]\n",
      "├─BatchNorm2d (bnorm_3): 1-16                 [1, 100, 42, 1]           [1, 100, 42, 1]           200                       --\n",
      "├─ELU (nonlin_3): 1-17                        [1, 100, 42, 1]           [1, 100, 42, 1]           --                        --\n",
      "├─MaxPool2d (pool_3): 1-18                    [1, 100, 42, 1]           [1, 100, 14, 1]           --                        [3, 1]\n",
      "├─Identity (pool_nonlin_3): 1-19              [1, 100, 14, 1]           [1, 100, 14, 1]           --                        --\n",
      "├─Dropout (drop_4): 1-20                      [1, 100, 14, 1]           [1, 100, 14, 1]           --                        --\n",
      "├─Conv2d (conv_4): 1-21                       [1, 100, 14, 1]           [1, 200, 5, 1]            200,000                   [10, 1]\n",
      "├─BatchNorm2d (bnorm_4): 1-22                 [1, 200, 5, 1]            [1, 200, 5, 1]            400                       --\n",
      "├─ELU (nonlin_4): 1-23                        [1, 200, 5, 1]            [1, 200, 5, 1]            --                        --\n",
      "├─MaxPool2d (pool_4): 1-24                    [1, 200, 5, 1]            [1, 200, 1, 1]            --                        [3, 1]\n",
      "├─Identity (pool_nonlin_4): 1-25              [1, 200, 1, 1]            [1, 200, 1, 1]            --                        --\n",
      "├─Sequential (final_layer): 1-26              [1, 200, 1, 1]            [1, 2]                    --                        --\n",
      "│    └─Conv2d (conv_classifier): 2-1          [1, 200, 1, 1]            [1, 2, 1, 1]              402                       [1, 1]\n",
      "│    └─SqueezeFinalOutput (squeeze): 2-2      [1, 2, 1, 1]              [1, 2]                    --                        --\n",
      "│    │    └─Rearrange (squeeze): 3-1          [1, 2, 1, 1]              [1, 2, 1]                 --                        --\n",
      "=================================================================================================================================================\n",
      "Total params: 343,927\n",
      "Trainable params: 343,927\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 5.03\n",
      "=================================================================================================================================================\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 0.30\n",
      "Params size (MB): 1.05\n",
      "Estimated Total Size (MB): 1.62\n",
      "=================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from braindecode.models import Deep4Net\n",
    "model = Deep4Net(n_chans=128, n_outputs=2, n_times=500)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47285a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True, False, False,  True, False, False,  True, False, False],\n",
      "        [ True,  True, False,  True, False,  True,  True,  True,  True,  True],\n",
      "        [False,  True, False, False,  True,  True,  True, False, False,  True],\n",
      "        [ True,  True, False, False,  True,  True, False, False,  True,  True],\n",
      "        [ True,  True,  True, False, False, False, False,  True, False, False],\n",
      "        [ True, False,  True, False,  True,  True,  True, False,  True,  True],\n",
      "        [ True, False,  True,  True, False, False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True, False,  True, False, False, False],\n",
      "        [ True,  True, False,  True,  True,  True, False, False,  True,  True],\n",
      "        [ True, False, False, False,  True, False, False,  True, False,  True],\n",
      "        [ True, False, False, False,  True,  True, False,  True, False,  True],\n",
      "        [False,  True, False, False,  True,  True,  True,  True,  True, False],\n",
      "        [False, False,  True, False,  True, False, False,  True, False,  True],\n",
      "        [False,  True, False, False,  True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False,  True,  True, False, False,  True, False],\n",
      "        [False, False,  True, False, False,  True,  True,  True, False,  True],\n",
      "        [False, False,  True,  True,  True,  True,  True, False, False, False],\n",
      "        [ True,  True, False,  True,  True,  True, False,  True, False, False],\n",
      "        [False,  True, False,  True, False, False, False, False,  True,  True],\n",
      "        [False, False,  True, False,  True, False,  True,  True, False,  True]])\n",
      "[tensor([2, 3, 5, 6, 8, 9]), tensor([2, 4]), tensor([0, 2, 3, 7, 8]), tensor([2, 3, 6, 7]), tensor([3, 4, 5, 6, 8, 9]), tensor([1, 3, 7]), tensor([1, 4, 5]), tensor([0, 1, 5, 7, 8, 9]), tensor([2, 6, 7]), tensor([1, 2, 3, 5, 6, 8]), tensor([1, 2, 3, 6, 8]), tensor([0, 2, 3, 9]), tensor([0, 1, 3, 5, 6, 8]), tensor([0, 2, 3]), tensor([2, 3, 6, 7, 9]), tensor([0, 1, 3, 4, 8]), tensor([0, 1, 7, 8, 9]), tensor([2, 6, 8, 9]), tensor([0, 2, 4, 5, 6, 7]), tensor([0, 1, 3, 5, 8])]\n",
      "tensor([0, 3, 2, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([8, 9, 3, 3, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "mask = (torch.rand((20,10)) < 0.5).bool() \n",
    "print(mask)\n",
    "def get_nonzero_indices_batch(input_tensor):\n",
    "  \"\"\"\n",
    "  Gets the indices of False elements along the batch dimension (dimension 0).\n",
    "\n",
    "  Args:\n",
    "    input_tensor: A PyTorch tensor.\n",
    "\n",
    "  Returns:\n",
    "    A list of LongTensors, where each LongTensor contains the indices\n",
    "    along the batch dimension where nonzero elements exist for the\n",
    "    corresponding element in the remaining dimensions.\n",
    "  \"\"\"\n",
    "  nonzero_indices_list = []\n",
    "  # Iterate through each element along the batch dimension\n",
    "  for i in range(input_tensor.size(0)):\n",
    "    nonzero_indices = torch.nonzero(~input_tensor[i])[:, 0]\n",
    "    nonzero_indices_list.append(nonzero_indices)\n",
    "  return nonzero_indices_list\n",
    "\n",
    "non_masked_inds = get_nonzero_indices_batch(mask)\n",
    "print(non_masked_inds)\n",
    "print(torch.randint(0, non_masked_inds[0].shape[0], (5,)))\n",
    "non_masked_inds[0][torch.randint(0, non_masked_inds[0].shape[0], (5,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d719d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_negatives_from_sequence(z, mask_t, num_negatives=5):\n",
    "        \"\"\"Generate negative samples from the encoder output of the same sequence but different time steps\"\"\"\n",
    "        batch_size, feat, full_len = z.shape\n",
    "        assert mask_t.shape == (batch_size, full_len), f\"mask_t {mask_t.shape} should be (B, seq_len)\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # gather False indices in mask_t for each batch item\n",
    "            non_masked_inds = [torch.nonzero(~item)[:, 0] for item in mask_t]\n",
    "            # for each batch item, sample with replacement self.num_negatives samples from negative_inds\n",
    "            negatives_inds = []\n",
    "            for i in range(batch_size):\n",
    "                negatives_inds.append(non_masked_inds[i][torch.randint(0, non_masked_inds[i].shape[0], (num_negatives,))])\n",
    "            negatives_inds = torch.stack(negatives_inds, dim=0) # (B, num_negatives)\n",
    "            assert negatives_inds.shape == (batch_size, num_negatives), f\"negatives indices {negatives_inds.shape} should be (B, num_negatives)\"\n",
    "            assert ~torch.any(torch.gather(mask, dim=1, index=negatives_inds)), f\"mask[negative_inds] should be False\"\n",
    "            \n",
    "            z_k = z.clone().permute([0, 2, 1]) # (B, seq_len, F)\n",
    "            negatives = torch.gather(z_k, dim=1, index=negatives_inds.unsqueeze(-1).expand(-1, -1, feat)) # (B, num_negatives, F)\n",
    "            negatives = negatives.permute([0, 2, 1]) # (B, F, num_negatives)\n",
    "            assert negatives.shape == (batch_size, feat, num_negatives), f\"negatives {negatives.shape} should be (B, F, num_negatives)\"\n",
    "\n",
    "            return negatives, negatives_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ff26821",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _ = _generate_negatives_from_sequence(torch.randn((20,10,10)), mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1348bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projection_head [Parameter containing:\n",
      "tensor([[ 0.1709,  0.2581, -0.3148,  0.1944, -0.2157, -0.1433,  0.2040, -0.0190,\n",
      "          0.2481, -0.0744],\n",
      "        [ 0.1580,  0.2435, -0.1837, -0.0369, -0.3125,  0.1737, -0.1756,  0.0784,\n",
      "         -0.2865, -0.2332],\n",
      "        [-0.2255,  0.0651,  0.2813,  0.1326, -0.0558,  0.3159, -0.1382, -0.1883,\n",
      "         -0.1853, -0.0718],\n",
      "        [ 0.0339, -0.0989, -0.1561, -0.1612, -0.0054,  0.2111, -0.1528, -0.0160,\n",
      "          0.1757, -0.3142],\n",
      "        [ 0.2914, -0.0747,  0.0996,  0.0055, -0.0389, -0.1731, -0.1679, -0.2122,\n",
      "          0.2429, -0.3097],\n",
      "        [ 0.0695, -0.2420, -0.0758,  0.0221, -0.2116,  0.1836,  0.3137, -0.0517,\n",
      "         -0.1626, -0.1631],\n",
      "        [-0.0165,  0.1339, -0.1492,  0.2247, -0.2080,  0.2238, -0.1984,  0.1170,\n",
      "          0.2981,  0.0101],\n",
      "        [-0.1110, -0.1754,  0.0034, -0.2797, -0.0463,  0.1407, -0.2960,  0.0853,\n",
      "          0.2184,  0.1719],\n",
      "        [ 0.1851, -0.1137,  0.0795, -0.2013, -0.0048, -0.3054,  0.2835, -0.1286,\n",
      "          0.2743, -0.2726],\n",
      "        [-0.1117,  0.2687, -0.2554, -0.2294, -0.2661,  0.2988, -0.0863, -0.2900,\n",
      "          0.1084, -0.1591],\n",
      "        [ 0.2334,  0.3069, -0.2886, -0.1042, -0.1700, -0.0575, -0.2347,  0.1958,\n",
      "         -0.2260, -0.0184],\n",
      "        [-0.0362, -0.2356, -0.1853,  0.1946,  0.1397, -0.0346,  0.1886,  0.0286,\n",
      "         -0.1083,  0.0755],\n",
      "        [-0.1822,  0.1680, -0.1521,  0.2228,  0.2361, -0.0737,  0.1627,  0.0869,\n",
      "         -0.2134, -0.2577],\n",
      "        [-0.0869, -0.0268,  0.0331,  0.0963,  0.1318, -0.2208,  0.2607, -0.1882,\n",
      "          0.0211, -0.1587],\n",
      "        [-0.1491, -0.2777,  0.1135, -0.1839,  0.1793, -0.2245, -0.1539,  0.0407,\n",
      "         -0.0743, -0.0725],\n",
      "        [ 0.1629,  0.2713,  0.0951, -0.1032,  0.0229,  0.2923, -0.2010, -0.0237,\n",
      "          0.2878,  0.1173],\n",
      "        [-0.0374, -0.2040, -0.1538,  0.0796, -0.2726, -0.0208, -0.0888, -0.2921,\n",
      "         -0.0508,  0.1275],\n",
      "        [ 0.0138,  0.1521,  0.1524, -0.0509, -0.2420,  0.0246,  0.2757,  0.0294,\n",
      "          0.1584,  0.1321],\n",
      "        [-0.0708, -0.0859,  0.0145,  0.2805, -0.1027, -0.2806,  0.0801, -0.0059,\n",
      "         -0.3149, -0.2428],\n",
      "        [ 0.1922, -0.1746, -0.1191, -0.1061, -0.1028, -0.0559,  0.2679, -0.2606,\n",
      "         -0.0928,  0.2275],\n",
      "        [-0.0892, -0.0157, -0.2640, -0.2196,  0.3003,  0.1424,  0.2674, -0.1481,\n",
      "          0.3012,  0.2301],\n",
      "        [ 0.0389, -0.1404,  0.1496,  0.3154,  0.1626, -0.2667,  0.2662,  0.1354,\n",
      "         -0.2087, -0.1799],\n",
      "        [-0.2195, -0.1263,  0.2251,  0.3050, -0.2462,  0.0329, -0.1522,  0.2492,\n",
      "         -0.0171, -0.2031],\n",
      "        [ 0.1078, -0.2516,  0.1240,  0.2542,  0.0714, -0.0581,  0.2875,  0.0680,\n",
      "         -0.2461, -0.0772],\n",
      "        [ 0.0196,  0.2891, -0.2465,  0.1770,  0.2491,  0.1218, -0.3116, -0.0256,\n",
      "          0.2099,  0.2978],\n",
      "        [-0.1636, -0.1811,  0.0632, -0.2814,  0.2401, -0.3147, -0.2430,  0.0036,\n",
      "         -0.2308, -0.1477],\n",
      "        [-0.1421,  0.1145, -0.2613, -0.1189,  0.0842,  0.1728, -0.1072, -0.2143,\n",
      "          0.3159, -0.0493],\n",
      "        [ 0.2414,  0.2756, -0.1518,  0.0965,  0.2802, -0.0163, -0.1244, -0.1518,\n",
      "          0.0298,  0.2397],\n",
      "        [-0.2172,  0.1371, -0.1346,  0.0027, -0.2776,  0.1758,  0.0227,  0.0533,\n",
      "          0.1529, -0.2692],\n",
      "        [ 0.1388, -0.2649, -0.0370, -0.0283,  0.2830, -0.0292,  0.2128, -0.3162,\n",
      "         -0.2124, -0.2745],\n",
      "        [-0.0604, -0.0437, -0.0045,  0.0029, -0.0630, -0.0244, -0.2850,  0.0008,\n",
      "         -0.2264, -0.2421],\n",
      "        [ 0.2492, -0.1711, -0.0990, -0.0395, -0.2435, -0.2416, -0.2961, -0.2087,\n",
      "          0.0162,  0.3153],\n",
      "        [ 0.2198,  0.2441,  0.2562,  0.1555, -0.0934, -0.0480, -0.1787,  0.0778,\n",
      "         -0.0256, -0.2094],\n",
      "        [-0.2300, -0.2410, -0.0142, -0.0106,  0.0925, -0.0605,  0.2880,  0.2028,\n",
      "         -0.2659, -0.2054],\n",
      "        [-0.0356,  0.2402, -0.0733,  0.2710, -0.0416,  0.2972, -0.1694,  0.0343,\n",
      "          0.3049, -0.2833],\n",
      "        [-0.2344, -0.1224,  0.0226,  0.2567, -0.0562, -0.0496, -0.0796,  0.2673,\n",
      "         -0.1950,  0.0128],\n",
      "        [ 0.0273, -0.0543, -0.2692, -0.1182, -0.1018, -0.0589, -0.0888, -0.0289,\n",
      "         -0.2276,  0.1360],\n",
      "        [-0.1859, -0.1212, -0.0595, -0.0844,  0.1296,  0.2744,  0.2238,  0.0985,\n",
      "          0.0349,  0.0098],\n",
      "        [-0.1655,  0.1437,  0.1650,  0.2188, -0.1892, -0.1147, -0.2405,  0.0620,\n",
      "         -0.1678,  0.0538],\n",
      "        [-0.1800,  0.1092, -0.1785, -0.0030,  0.2652,  0.0788, -0.0330,  0.1548,\n",
      "          0.1972,  0.0803],\n",
      "        [-0.1532,  0.2003, -0.2887, -0.1122, -0.0243,  0.1189, -0.1789, -0.1628,\n",
      "          0.0176, -0.0484],\n",
      "        [ 0.2606,  0.2051, -0.0564, -0.3119, -0.0778,  0.1701,  0.1778, -0.1383,\n",
      "          0.2419, -0.2068],\n",
      "        [ 0.2393, -0.1113,  0.2252, -0.1947, -0.1830, -0.1740, -0.1012, -0.0470,\n",
      "         -0.1186,  0.0825],\n",
      "        [-0.0139,  0.2289, -0.1062,  0.0555, -0.2484, -0.2730, -0.0090, -0.1709,\n",
      "          0.1810, -0.0777],\n",
      "        [ 0.0620, -0.2055, -0.0135,  0.0249, -0.2323, -0.0813,  0.2608,  0.0114,\n",
      "         -0.1321, -0.1013],\n",
      "        [ 0.3114,  0.2533,  0.1658,  0.0637,  0.2125, -0.1107,  0.1496,  0.1789,\n",
      "          0.1813, -0.3090],\n",
      "        [-0.1076,  0.1144, -0.1378,  0.0185,  0.2699,  0.1969,  0.1663, -0.2319,\n",
      "         -0.0942,  0.1813],\n",
      "        [-0.1969,  0.1874, -0.2159,  0.0755,  0.0920, -0.1438,  0.1442, -0.0824,\n",
      "         -0.1057, -0.0163],\n",
      "        [-0.1375,  0.2573,  0.0039,  0.0052,  0.1044, -0.1188, -0.2148, -0.1835,\n",
      "          0.2460,  0.3004],\n",
      "        [ 0.1142,  0.0331, -0.2308,  0.0544, -0.1502,  0.2514,  0.2085,  0.0607,\n",
      "          0.0435,  0.0569],\n",
      "        [ 0.0235,  0.0277, -0.1190,  0.2683, -0.2642,  0.2062, -0.0231,  0.0058,\n",
      "         -0.3001,  0.0651],\n",
      "        [ 0.0929, -0.1876, -0.1337, -0.2176, -0.2339,  0.2532,  0.2525, -0.1600,\n",
      "         -0.0791,  0.1409],\n",
      "        [-0.1072,  0.1949,  0.2640,  0.0967, -0.1238, -0.0423, -0.2183, -0.0685,\n",
      "         -0.2635, -0.1890],\n",
      "        [ 0.1714, -0.1418, -0.2144, -0.3118,  0.1679, -0.0022,  0.1168, -0.2502,\n",
      "         -0.1385,  0.1740],\n",
      "        [ 0.2401,  0.1413,  0.0387,  0.2778, -0.0559, -0.0118, -0.2122, -0.3162,\n",
      "          0.0193,  0.1054],\n",
      "        [-0.0338,  0.0869, -0.2151,  0.2909,  0.3079,  0.0488, -0.1997, -0.2179,\n",
      "          0.1227,  0.0549],\n",
      "        [-0.0700,  0.1255, -0.0938, -0.1116, -0.1502, -0.1000, -0.2906, -0.2000,\n",
      "          0.0273,  0.2525],\n",
      "        [ 0.1316, -0.3072, -0.2472, -0.0465,  0.1771, -0.2575,  0.0282, -0.2484,\n",
      "         -0.1315, -0.1230],\n",
      "        [ 0.0270,  0.1386,  0.1354, -0.3026,  0.2067,  0.0468, -0.0073,  0.2227,\n",
      "          0.0817, -0.0374],\n",
      "        [-0.0758,  0.0062,  0.1470, -0.1055,  0.0435,  0.2993,  0.0433,  0.0876,\n",
      "         -0.3036,  0.1593],\n",
      "        [ 0.1512, -0.0357, -0.0904,  0.3161,  0.0663,  0.1971,  0.1158,  0.2827,\n",
      "         -0.0275, -0.2888],\n",
      "        [ 0.2495, -0.3162, -0.2423,  0.0531,  0.1415,  0.2991, -0.0945,  0.0526,\n",
      "          0.0649,  0.2547],\n",
      "        [-0.1064, -0.1665,  0.2883,  0.1653,  0.2383, -0.1217,  0.0284, -0.2238,\n",
      "         -0.2742,  0.2097],\n",
      "        [-0.1351,  0.0955,  0.1862,  0.0565, -0.1694,  0.0072,  0.2101, -0.0560,\n",
      "         -0.1323,  0.0068],\n",
      "        [-0.2808, -0.0303, -0.2249, -0.1984,  0.2652, -0.1305, -0.0743, -0.1149,\n",
      "         -0.0592,  0.2429],\n",
      "        [ 0.2192, -0.0027, -0.3016, -0.2546,  0.1013,  0.1779,  0.0859,  0.1449,\n",
      "         -0.1934,  0.0361],\n",
      "        [-0.1057, -0.0439, -0.0840,  0.0650,  0.1548, -0.0496,  0.0353, -0.1722,\n",
      "         -0.2489, -0.2473],\n",
      "        [-0.1722, -0.0566, -0.1386,  0.2016, -0.2751, -0.2101,  0.0062,  0.1058,\n",
      "         -0.0686,  0.0342],\n",
      "        [ 0.1388, -0.0499,  0.2638,  0.0441,  0.2739,  0.1675,  0.2094, -0.0049,\n",
      "         -0.2141, -0.1583],\n",
      "        [-0.2636,  0.3109,  0.2620,  0.1736, -0.1498, -0.0429, -0.0182, -0.2522,\n",
      "          0.0647,  0.1035],\n",
      "        [-0.0245, -0.0252,  0.1016,  0.1205, -0.2546,  0.0202,  0.1763, -0.1591,\n",
      "         -0.1319,  0.0325],\n",
      "        [ 0.0098, -0.1847, -0.1341, -0.0357,  0.0425,  0.2922,  0.0921,  0.1933,\n",
      "         -0.0271, -0.2806],\n",
      "        [ 0.1784, -0.1518,  0.1668,  0.2949,  0.2174, -0.2311,  0.0550, -0.1869,\n",
      "         -0.2465,  0.0482],\n",
      "        [ 0.3038, -0.0730,  0.2701,  0.2830,  0.2287, -0.0645,  0.1958, -0.2427,\n",
      "         -0.1775,  0.1202],\n",
      "        [ 0.1512, -0.0679, -0.1207, -0.1815,  0.0980,  0.2651,  0.1682,  0.0928,\n",
      "          0.0200,  0.2244],\n",
      "        [ 0.0246,  0.1420,  0.3117,  0.2796,  0.0897, -0.1798,  0.1076,  0.1669,\n",
      "          0.1267, -0.2827],\n",
      "        [-0.2875, -0.0429, -0.2138,  0.0682,  0.0781,  0.0269, -0.2837,  0.1385,\n",
      "         -0.2607, -0.1546],\n",
      "        [ 0.0896,  0.1242, -0.1861, -0.1275,  0.0561,  0.2721,  0.1530, -0.3065,\n",
      "          0.1415,  0.2775],\n",
      "        [-0.0849, -0.0464, -0.0415,  0.0197,  0.1908, -0.2800, -0.2822,  0.3062,\n",
      "         -0.0744, -0.1418],\n",
      "        [ 0.0721, -0.2672, -0.1589, -0.2156, -0.2071,  0.1583,  0.1756,  0.1053,\n",
      "          0.0542,  0.1619],\n",
      "        [-0.0584, -0.1614, -0.0192,  0.0667,  0.2424,  0.2746,  0.0542, -0.2443,\n",
      "          0.1069,  0.0726],\n",
      "        [-0.3051,  0.1488,  0.2753, -0.2032, -0.0859,  0.1975,  0.1738,  0.2955,\n",
      "          0.2695,  0.3111],\n",
      "        [-0.2045, -0.0401,  0.2340, -0.2100,  0.2125, -0.2776,  0.3131,  0.1785,\n",
      "          0.1205, -0.1540],\n",
      "        [-0.0857, -0.0306, -0.2119,  0.2375,  0.0655,  0.1248, -0.0004, -0.2409,\n",
      "          0.2531, -0.1439],\n",
      "        [-0.2651, -0.3121,  0.3086,  0.2709, -0.0237,  0.0208, -0.0632, -0.3029,\n",
      "          0.1857, -0.2071],\n",
      "        [-0.1319,  0.1511, -0.0965,  0.1489, -0.0460, -0.2051,  0.1710, -0.1765,\n",
      "          0.1109, -0.2380],\n",
      "        [ 0.2121, -0.2687,  0.2152,  0.1461,  0.1399, -0.1930,  0.0042,  0.1920,\n",
      "          0.0896, -0.2232],\n",
      "        [ 0.1229,  0.0169, -0.0322, -0.0036, -0.0360,  0.1480, -0.2965, -0.1502,\n",
      "         -0.1521, -0.0509],\n",
      "        [ 0.0387,  0.1970, -0.0216, -0.0885, -0.1985, -0.0209, -0.3127,  0.0899,\n",
      "          0.0115,  0.2680],\n",
      "        [ 0.2931,  0.1186,  0.2548, -0.0112, -0.1069, -0.1654, -0.1928, -0.1413,\n",
      "          0.2188,  0.0280],\n",
      "        [ 0.1728, -0.2377, -0.0749, -0.2133, -0.2160, -0.0060, -0.0024, -0.0201,\n",
      "          0.2386,  0.2416],\n",
      "        [-0.0053,  0.2704,  0.0813, -0.0234, -0.2185, -0.2482,  0.0747, -0.1587,\n",
      "         -0.2026, -0.0446],\n",
      "        [-0.2731, -0.0352, -0.3013,  0.2463,  0.0669,  0.0024, -0.2181, -0.0015,\n",
      "          0.1797,  0.1734],\n",
      "        [ 0.2940, -0.2034, -0.3044,  0.0868, -0.2388, -0.0562, -0.2957,  0.0588,\n",
      "          0.3084, -0.1432],\n",
      "        [-0.1544,  0.2535, -0.0179,  0.2880,  0.2331, -0.2574,  0.1673, -0.1299,\n",
      "         -0.2234, -0.3118],\n",
      "        [-0.1050, -0.1121,  0.2419,  0.1248, -0.1815, -0.2919, -0.1684,  0.0282,\n",
      "          0.1414,  0.1916],\n",
      "        [-0.0142,  0.0535,  0.1228,  0.2251,  0.1765, -0.2450, -0.1622, -0.3009,\n",
      "         -0.0890, -0.2750],\n",
      "        [ 0.0982, -0.2682, -0.2264,  0.1059, -0.2552, -0.1690,  0.0898, -0.2131,\n",
      "         -0.1224,  0.2216],\n",
      "        [-0.3093,  0.0132, -0.1292,  0.1759,  0.1537, -0.0129,  0.2332, -0.2634,\n",
      "          0.2580,  0.1904],\n",
      "        [ 0.0215, -0.1954, -0.0202, -0.2757, -0.0520,  0.1393, -0.0381,  0.0048,\n",
      "         -0.1414,  0.0758]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.1034, -0.1930,  0.1282, -0.2195,  0.0951, -0.1153,  0.0476, -0.0610,\n",
      "        -0.3095,  0.1160, -0.0368,  0.2401, -0.1876, -0.0582, -0.2811,  0.1207,\n",
      "         0.2807, -0.0367, -0.1104,  0.0310,  0.2935,  0.2948,  0.1795, -0.1133,\n",
      "         0.0267, -0.2403,  0.2670,  0.2422, -0.1633,  0.0772,  0.2722,  0.1162,\n",
      "        -0.0162, -0.2624,  0.0401, -0.0311,  0.1471, -0.0334,  0.1867,  0.2086,\n",
      "        -0.0378,  0.2005, -0.2466,  0.1185, -0.1804, -0.1739,  0.0999,  0.2659,\n",
      "         0.0798,  0.1945,  0.0053,  0.1118,  0.2769, -0.1579, -0.0371,  0.1191,\n",
      "         0.0469,  0.0050, -0.2941,  0.0173,  0.0987, -0.1136, -0.1074,  0.1397,\n",
      "        -0.3086,  0.0621,  0.1630,  0.0142,  0.3123, -0.1232, -0.0484, -0.1271,\n",
      "        -0.1713, -0.2619, -0.1023, -0.1908, -0.1963,  0.1842,  0.1627,  0.0892,\n",
      "        -0.1783,  0.1281,  0.1391,  0.0112,  0.0849, -0.0310,  0.1436, -0.2716,\n",
      "         0.1799, -0.2949,  0.2964, -0.0670, -0.0540,  0.1737, -0.1621,  0.2450,\n",
      "        -0.0254,  0.2770,  0.1322, -0.1707], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.0044,  0.0733,  0.0959, -0.0300,  0.0367,  0.0284,  0.0991, -0.0246,\n",
      "          0.0483,  0.0096,  0.0900, -0.0213,  0.0594,  0.0597,  0.0088,  0.0063,\n",
      "         -0.0683, -0.0462, -0.0692, -0.0609,  0.0454,  0.0939,  0.0686,  0.0959,\n",
      "          0.0959, -0.0525,  0.0012,  0.0134, -0.0789, -0.0525,  0.0703, -0.0005,\n",
      "         -0.0355,  0.0508,  0.0938, -0.0286,  0.0749,  0.0407,  0.0996, -0.0359,\n",
      "          0.0243, -0.0352,  0.0579, -0.0081,  0.0923, -0.0190, -0.0748,  0.0545,\n",
      "          0.0343, -0.0197, -0.0038,  0.0808, -0.0835, -0.0067, -0.0789,  0.0329,\n",
      "         -0.0062,  0.0651, -0.0175, -0.0975,  0.0256,  0.0094, -0.0674, -0.0034,\n",
      "          0.0908,  0.0008, -0.0265, -0.0089, -0.0703,  0.0969, -0.0383, -0.0105,\n",
      "          0.0116,  0.0266, -0.0778, -0.0258, -0.0413,  0.0983,  0.0053,  0.0432,\n",
      "          0.0234,  0.0581,  0.0892,  0.0347, -0.0418,  0.0802,  0.0800,  0.0694,\n",
      "         -0.0195, -0.0572,  0.0306,  0.0313,  0.0716,  0.0644, -0.0037, -0.0836,\n",
      "          0.0214,  0.0558,  0.0551, -0.0930]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0496], requires_grad=True)]\n",
      "SimpleNet(\n",
      "  (fc1): Linear(in_features=10, out_features=100, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.8411, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.8100, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.7811, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.7545, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.7301, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.7079, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.6877, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.6696, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.6532, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.6385, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.6253, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.6135, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.6027, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5928, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5836, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5749, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5666, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5586, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5509, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5433, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5359, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5285, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5212, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5140, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.5068, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4997, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4927, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4858, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4790, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4724, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4658, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4595, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4533, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4472, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4413, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4356, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4300, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4246, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4192, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4140, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4088, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.4038, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3988, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3938, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3889, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3840, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3792, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3744, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3697, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3651, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3604, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3559, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3513, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3469, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3425, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3381, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3338, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3296, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3254, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3213, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3173, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3133, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3093, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3053, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.3014, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2975, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2937, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2899, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2861, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2824, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2787, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2751, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2715, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2679, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2644, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2609, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2574, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2539, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2505, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2471, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2437, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2404, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2371, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2338, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2306, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2274, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2242, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2211, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2179, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2148, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2117, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2087, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2057, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.2027, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.1997, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.1968, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.1940, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.1911, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.1882, grad_fn=<MseLossBackward0>)\n",
      "preds torch.Size([32, 1])\n",
      "labels torch.Size([32, 1])\n",
      "loss tensor(0.1854, grad_fn=<MseLossBackward0>)\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# Example of using it in your original code.\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#  Fix: Ensure x and labels are defined.\n",
    "input_dim = 10\n",
    "batch_size = 32\n",
    "x = torch.randn(batch_size, input_dim) #  <---  No requires_grad here initially\n",
    "labels = torch.randn(batch_size, 1, requires_grad=False)\n",
    "\n",
    "# Set requires_grad to True AFTER x is created.\n",
    "# x.requires_grad_(True)\n",
    "\n",
    "projection_head = SimpleNet(input_dim).to(x.device)\n",
    "print('projection_head', list(projection_head.parameters()))\n",
    "projection_head.train()\n",
    "optimizer = optim.Adam(projection_head.parameters(), lr=0.001)\n",
    "print(projection_head)\n",
    "\n",
    "for e in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    preds = projection_head(x)\n",
    "    print('preds', preds.shape)\n",
    "    print('labels', labels.shape)\n",
    "    loss = F.mse_loss(preds, labels)\n",
    "    print('loss', loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ffc4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
