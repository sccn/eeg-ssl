{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8205fe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from libs.ssl_task import CPC\n",
    "from libs.ssl_data import SSLHBNDataModule\n",
    "import torch\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d403b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config from runs/config_CPC.yaml\n",
    "import yaml\n",
    "with open('../runs/config_CPC.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "731b85c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl_task = CPC()\n",
    "config['data']['ssl_task'] = ssl_task\n",
    "config['data']['window_len_s'] = 20\n",
    "config['data']['num_workers'] = 2\n",
    "litDataModule = SSLHBNDataModule(**config['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8804eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.ssl_task import LitSSL\n",
    "from libs.ssl_utils import instantiate_module\n",
    "from libs.ssl_model import BENDRLSTM, BENDRContextualizer\n",
    "from libs.evaluation import Regressor, RankMe\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "from typing import Any, Optional, Union\n",
    "class CPCLit(LitSSL):\n",
    "    # Repurpose from https://github.com/SPOClab-ca/BENDR/blob/ac918abaec111d15fcaa2a8fcd2bd3d8b0d81a10/dn3_ext.py#L232\n",
    "    def __init__(self, \n",
    "                contextualizer_path: str,\n",
    "                contextualizer_kwargs: Optional[Union[dict[str, Any], dict[str, dict[str, Any]]]] = None, \n",
    "                downsampling_factor=96, \n",
    "                mask_rate=0.1, mask_span=6, temp=0.1,\n",
    "                permuted_encodings=False, permuted_contexts=False, enc_feat_l2=0.001,\n",
    "                unmasked_negative_frac=0.25, num_negatives=20, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # self.contextualizer = BENDRContextualizer(\n",
    "        #     in_features=self.encoder_emb_size,\n",
    "        #     start_token=None,\n",
    "        # )\n",
    "        self.contextualizer = instantiate_module(contextualizer_path, contextualizer_kwargs)\n",
    "        # Initialize replacement vector with standard normal\n",
    "        self.mask_replacement = torch.nn.Parameter(torch.normal(0, self.encoder_emb_size**(-0.5), size=(self.encoder_emb_size,)),\n",
    "                                                requires_grad=True)\n",
    "\n",
    "        self.predict_length = mask_span\n",
    "        self._enc_downsample = downsampling_factor\n",
    "        self.mask_rate = mask_rate\n",
    "        self.mask_span = mask_span\n",
    "        self.temp = temp\n",
    "        self.permuted_encodings = permuted_encodings\n",
    "        self.permuted_contexts = permuted_contexts\n",
    "        self.beta = enc_feat_l2\n",
    "        self.start_token = getattr(self.contextualizer, 'start_token', None)\n",
    "        self.unmasked_negative_frac = unmasked_negative_frac\n",
    "        self.num_negatives = num_negatives\n",
    "\n",
    "        self.evaluators = [RankMe(), Regressor(projection_head=True)]\n",
    "    \n",
    "    def _generate_negatives(self, z):\n",
    "        \"\"\"Generate negative samples to compare each sequence location against\"\"\"\n",
    "        batch_size, feat, full_len = z.shape\n",
    "        with torch.no_grad():\n",
    "            z_k = z.clone().permute([0, 2, 1]).reshape(-1, feat)\n",
    "            negative_inds = torch.randint(0, full_len-1, size=(batch_size, full_len * self.num_negatives))\n",
    "            # From wav2vec 2.0 implementation, I don't understand\n",
    "            # negative_inds[negative_inds >= candidates] += 1\n",
    "\n",
    "            for i in range(1, batch_size):\n",
    "                negative_inds[i] += i * full_len\n",
    "\n",
    "            z_k = z_k[negative_inds.view(-1)].view(batch_size, full_len, self.num_negatives, feat)\n",
    "            return z_k, negative_inds\n",
    "\n",
    "    def _calculate_similarity(self, true_z, c, negatives):\n",
    "        targets = true_z.permute([0, 2, 1]).unsqueeze(-2)\n",
    "        # z - (B, seq_len, 1, F) - YT\n",
    "\n",
    "        if self.start_token:\n",
    "            c = c[..., 1:].permute([0, 2, 1]).unsqueeze(-2)\n",
    "        else:\n",
    "            c = c.permute([0, 2, 1]).unsqueeze(-2)\n",
    "        # c - (B, seq_len, 1, F). First seq is added start token - YT\n",
    "        # negatives - (B, seq_len, num_negatives, F) - YT\n",
    "        predictions = torch.cat([c, negatives], dim=-2)\n",
    "        # predictions - (B, seq_len, 1+num_negatives, F) - YT\n",
    "\n",
    "        logits = F.cosine_similarity(targets, predictions, dim=-1) / self.temp # z is being broadcasted in the 3rd dimension - YT\n",
    "        # logits - (B, seq_len, 1+num_negatives)\n",
    "\n",
    "        return logits.view(-1, logits.shape[-1]) # flatten B x seq_len. Last dim correspond to torch CrossEntropyLoss C \n",
    "                                                    # --> will have true class label 0\n",
    "\n",
    "    def compute_cross_batch_loss(self, true_z, c):\n",
    "        B, feat, seq_len = true_z.shape\n",
    "        assert c.shape == true_z.shape, f\"c {c.shape} and true_z {true_z.shape} should be the same shape\"\n",
    "        true_z = true_z.permute([0, 2, 1]) # (B, seq_len, F)\n",
    "        c = c.permute([0, 2, 1]) # (B, seq_len, F)\n",
    "\n",
    "        positives = F.cosine_similarity(c, true_z, dim=-1) / self.temp # (B, seq_len)\n",
    "        # create negative batch by randomize batch elements\n",
    "        negatives_batch_ind = torch.randint(0, B, (B,), device=true_z.device)\n",
    "        negatives_batch = true_z[negatives_batch_ind] # (B, seq_len, F)\n",
    "        assert negatives_batch.shape == true_z.shape, f\"negatives_batch {negatives_batch.shape} should be the same shape as true_z {true_z.shape}\"\n",
    "        negatives_seq_ind = torch.randint(0, seq_len, (B, seq_len, self.num_negatives), device=true_z.device)\n",
    "        negatives_batch = torch.gather(negatives_batch.unsqueeze(2).expand(-1, -1, self.num_negatives, -1),  # Expand to (B, seq_len, num_negative, F)\n",
    "                                dim=1,  # Sample along the time dimension (originally dimension 1)\n",
    "                                index=negatives_seq_ind.unsqueeze(-1).expand(-1, -1, -1, feat))\n",
    "        negatives = F.cosine_similarity(c.unsqueeze(-2), negatives_batch, dim=-1) / self.temp # (B, seq_len, num_negatives)\n",
    "\n",
    "        # assert positives.shape == negatives.shape, f\"positives {positives.shape} and negatives {negatives.shape} should be the same shape\"\n",
    "        # assert not torch.allclose(positives, negatives), f\"positives {positives.shape} and negatives {negatives.shape} should not be the same\"\n",
    "\n",
    "        # negatives = negatives.unsqueeze(-1).expand(-1, -1, self.num_negatives)\n",
    "        assert negatives.shape == (B, seq_len, self.num_negatives), f\"negatives {negatives.shape} should be (B, seq_len, num_negatives)\"\n",
    "\n",
    "        positives = positives.unsqueeze(-1) # (B, seq_len, 1)\n",
    "        assert positives.shape == (B, seq_len, 1), f\"positives {positives.shape} should be (B, seq_len, 1)\"\n",
    "\n",
    "        logits = torch.cat([positives, negatives], dim=-1) # (B, seq_len, 1+num_negatives)\n",
    "        logits = logits * 10\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1]) # flatten B x seq_len. Last dim correspond to torch CrossEntropyLoss C \n",
    "                                                    # --> will have true class label 0\n",
    "        \n",
    "        labels = torch.zeros(logits.shape[0], device=logits.device, dtype=torch.long)\n",
    "        # labels - (B x seq_len) all 0s\n",
    "        # print('labels', labels)\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    # def calculate_loss(self, inputs, outputs):\n",
    "    #     logits = outputs[0]\n",
    "    #     labels = torch.zeros(logits.shape[0], device=logits.device, dtype=torch.long)\n",
    "    #     # Note the loss_fn here integrates the softmax as per the normal classification pipeline (leveraging logsumexp)\n",
    "    #     return torch.nn.functional.cross_entropy(logits, labels) + self.beta * outputs[1].pow(2).mean()\n",
    "\n",
    "    def _make_span_from_seeds(self, seeds, span, total=None):\n",
    "        inds = list()\n",
    "        for seed in seeds:\n",
    "            for i in range(seed, seed + span):\n",
    "                if total is not None and i >= total:\n",
    "                    break\n",
    "                elif i not in inds:\n",
    "                    inds.append(int(i))\n",
    "        return np.array(inds)\n",
    "\n",
    "    def _make_mask(self, shape, p, total, span, allow_no_inds=False):\n",
    "        # num_mask_spans = np.sum(np.random.rand(total) < p)\n",
    "        # num_mask_spans = int(p * total)\n",
    "        mask = torch.zeros(shape, requires_grad=False, dtype=torch.bool)\n",
    "\n",
    "        for i in range(shape[0]):\n",
    "            mask_seeds = list()\n",
    "            while not allow_no_inds and len(mask_seeds) == 0 and p > 0:\n",
    "                mask_seeds = np.nonzero(np.random.rand(total) < p)[0]\n",
    "\n",
    "            mask[i, self._make_span_from_seeds(mask_seeds, span, total=total)] = True\n",
    "        # mask - (B, seq_len)\n",
    "        return mask\n",
    "\n",
    "    def generate_negatives_from_batch(self, z):\n",
    "        \"\"\"Generate negatives from other samples in the batch\"\"\"\n",
    "        batch_size, feat, full_len = z.shape\n",
    "        with torch.no_grad():\n",
    "            z_k = z.clone().permute([0, 2, 1]).reshape(-1, feat)\n",
    "            negative_inds = torch.randint(0, full_len-1, size=(batch_size, full_len * self.num_negatives))\n",
    "            # From wav2vec 2.0 implementation, I don't understand\n",
    "            # negative_inds[negative_inds >= candidates] += 1\n",
    "\n",
    "            for i in range(1, batch_size):\n",
    "                negative_inds[i] += i * full_len\n",
    "\n",
    "            z_k = z_k[negative_inds.view(-1)].view(batch_size, full_len, self.num_negatives, feat)\n",
    "            return z_k, negative_inds\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        z = self.encoder(batch[0])\n",
    "        batch_size, feat, samples = z.shape\n",
    "        # z - (B, F, seq_len)\n",
    "\n",
    "        unmasked_z = z.clone()\n",
    "        \n",
    "        mask = None\n",
    "        mask = self._make_mask((batch_size, samples), self.mask_rate, samples, self.mask_span)\n",
    "        # make simple mask: only predict the last token\n",
    "        # mask = torch.zeros((batch_size, samples), dtype=torch.bool)\n",
    "        # mask[:, -1] = True\n",
    "\n",
    "        if mask is not None:\n",
    "            z.transpose(2, 1)[mask] = self.mask_replacement\n",
    "\n",
    "        c = self.contextualizer(z)\n",
    "        # c - (B, F, seq_len) \n",
    "\n",
    "        loss = self.compute_cross_batch_loss(unmasked_z, c)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        z = self.encoder(batch[0])\n",
    "        Y, subjects = batch[1], batch[3]\n",
    "        batch_size, feat, samples = z.shape\n",
    "        # z - (B, F, seq_len)\n",
    "\n",
    "        unmasked_z = z.clone()\n",
    "        \n",
    "        mask = None\n",
    "        # mask = self._make_mask((batch_size, samples), self.mask_rate, samples, self.mask_span)\n",
    "        # make simple mask: only predict the last token\n",
    "        # mask = torch.zeros((batch_size, samples), dtype=torch.bool)\n",
    "        # mask[:, -1] = True\n",
    "\n",
    "        if mask is not None:\n",
    "            z.transpose(2, 1)[mask] = self.mask_replacement\n",
    "\n",
    "        c = self.contextualizer(z)\n",
    "        \n",
    "        loss = self.compute_cross_batch_loss(unmasked_z, c)\n",
    "\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        for evaluator in self.evaluators:\n",
    "            c_last = c[:, :, -1]\n",
    "            evaluator.update((c_last, Y, subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2019aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPCLit(\n",
      "  (encoder): ConvEncoderBENDR(\n",
      "    (encoder): Sequential(\n",
      "      (Encoder_0): Sequential(\n",
      "        (0): Conv1d(128, 512, kernel_size=(3,), stride=(3,), padding=(1,))\n",
      "        (1): Dropout1d(p=0.1, inplace=False)\n",
      "        (2): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "        (3): GELU(approximate='none')\n",
      "      )\n",
      "      (Encoder_1): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "        (1): Dropout1d(p=0.1, inplace=False)\n",
      "        (2): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "        (3): GELU(approximate='none')\n",
      "      )\n",
      "      (Encoder_2): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "        (1): Dropout1d(p=0.1, inplace=False)\n",
      "        (2): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "        (3): GELU(approximate='none')\n",
      "      )\n",
      "      (Encoder_3): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "        (1): Dropout1d(p=0.1, inplace=False)\n",
      "        (2): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "        (3): GELU(approximate='none')\n",
      "      )\n",
      "      (Encoder_4): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "        (1): Dropout1d(p=0.1, inplace=False)\n",
      "        (2): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "        (3): GELU(approximate='none')\n",
      "      )\n",
      "      (Encoder_5): Sequential(\n",
      "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), padding=(1,))\n",
      "        (1): Dropout1d(p=0.1, inplace=False)\n",
      "        (2): GroupNorm(256, 512, eps=1e-05, affine=True)\n",
      "        (3): GELU(approximate='none')\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (contextualizer): BENDRLSTM(\n",
      "    (contextualizer): LSTM(512, 512, num_layers=3, batch_first=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config['model']['learning_rate'] = 0.005\n",
    "cpc_model = CPCLit(downsampling_factor=96, **config['model']['init_args'])\n",
    "print(cpc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "778d11fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using datasets: ['ds005505', 'ds005506', 'ds005507', 'ds005508', 'ds005509', 'ds005510', 'ds005511', 'ds005512', 'ds005514', 'ds005515', 'ds005516']\n",
      "Validation release: ds005505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | encoder        | ConvEncoderBENDR | 4.1 M  | train\n",
      "1 | contextualizer | BENDRLSTM        | 6.3 M  | train\n",
      "  | other params   | n/a              | 512    | n/a  \n",
      "------------------------------------------------------------\n",
      "10.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.4 M    Total params\n",
      "41.769    Total estimated model params size (MB)\n",
      "34        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datasets: 2698\n",
      "Number of examples: 53711\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fbacf2066f4860bb15dc9d800d46ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed24e17e564b432097eda4ddf1763406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Expect 2D embeddings of shape (N, K)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainer\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcpc_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitDataModule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:151\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:370\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.on_advance_end\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_accumulate():\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# clear gradients to not leave any unused memory during validation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_model_zero_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_first_loop_iter \u001b[38;5;241m=\u001b[39m first_loop_iter\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:151\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_iteration_done()\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_run_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:291\u001b[0m, in \u001b[0;36m_EvaluationLoop.on_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39m_evaluation_epoch_end()\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_evaluation_epoch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m logged_outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logged_outputs, []  \u001b[38;5;66;03m# free memory\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# include any logged outputs on epoch_end\u001b[39;00m\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/loops/evaluation_loop.py:371\u001b[0m, in \u001b[0;36m_EvaluationLoop._on_evaluation_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_epoch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(trainer, hook_name)\n\u001b[0;32m--> 371\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m trainer\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mon_epoch_end()\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    174\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/eeg-ssl/notebooks/../libs/ssl_utils.py:66\u001b[0m, in \u001b[0;36mLitSSL.on_validation_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_validation_epoch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m evaluator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluators:\n\u001b[0;32m---> 66\u001b[0m         val \u001b[38;5;241m=\u001b[39m  \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(val) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m val\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/eeg-ssl/.venv/lib/python3.10/site-packages/torchmetrics/metric.py:700\u001b[0m, in \u001b[0;36mMetric._wrap_compute.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;66;03m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;66;03m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_context(\n\u001b[1;32m    696\u001b[0m     dist_sync_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_sync_fn,\n\u001b[1;32m    697\u001b[0m     should_sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_sync,\n\u001b[1;32m    698\u001b[0m     should_unsync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_unsync,\n\u001b[1;32m    699\u001b[0m ):\n\u001b[0;32m--> 700\u001b[0m     value \u001b[38;5;241m=\u001b[39m _squeeze_if_scalar(\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;66;03m# clone tensor to avoid in-place operations after compute, altering already computed results\u001b[39;00m\n\u001b[1;32m    702\u001b[0m     value \u001b[38;5;241m=\u001b[39m apply_to_collection(value, Tensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mclone())\n",
      "File \u001b[0;32m~/eeg-ssl/notebooks/../libs/evaluation.py:30\u001b[0m, in \u001b[0;36mRankMe.compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m embs \u001b[38;5;241m=\u001b[39m dim_zero_cat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membs)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(embs\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpect 2D embeddings of shape (N, K)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRankMe embs shape\u001b[39m\u001b[38;5;124m'\u001b[39m, embs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m embs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: Expect 2D embeddings of shape (N, K)"
     ]
    }
   ],
   "source": [
    "config['trainer']['callbacks'] = None\n",
    "config['trainer']['logger'] = None\n",
    "config['trainer']['overfit_batches'] = 0.0\n",
    "config['trainer']['fast_dev_run'] = True\n",
    "config['trainer']['max_epochs'] = 100\n",
    "trainer = L.Trainer(**config['trainer'])\n",
    "trainer.fit(model=cpc_model, datamodule=litDataModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b323d228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
